{
  
    
        "post0": {
            "title": "Writing a Custom Metric for the fastai Library",
            "content": "In an earlier blog post I described how to classify images of distracted drivers using the fastai library. The dataset, that I used to train my classification model with, came from the Distracted Driver Kaggle competition. During model training I used the accuracy metric to report the performance of my model. However, if we take a closer look at the description of the Distracted Driver Kaggle competition, we can see that accuracy is actually not the evaluation metric that is supposed to be used for this competition. It is the multi-class logarithmic loss (log loss) instead! Wait! Don&#39;t we already use the log loss for optimizing our model? Yes, we do! But we can also use it as an evaluation metric. Kaggle actually does this for many of its competitions. Okay, but what&#39;s the difference compared to accuracy? Well, accuracy counts the predictions that were actually correct. You can find more information about it here and here. However, accuracy is not in every case a good evaluation metric because of its yes or no nature. The log loss on the other hand takes the uncertainty of the prediction into account regarding how far it is away from the actual target. You can find out more about it here and here. . So, let&#39;s use the log loss as our evaluation metric. However, when we take a look into fastai&#39;s documentation, we can see that fastai actually doesn&#39;t offer a predefined function calculating the log loss metric. Before, when we were using the accuracy, we could simply use the predefined accuracy metric function from fastai. Now, we have to implement the log loss metric by ourselves and add it to our learner. Well, actually we don&#39;t have to add the log loss as evaluation metric at all, since the log loss is also used for optimizing our model and thus fastai already shows it to us. Nevertheless, I still want to show how we can write a custom metric for fastai using the example of the log loss. Later we can use the same way as shown here to write other kinds of custom metrics for fastai. . However, before we implement our custom metric let&#39;s make sure we have PyTorch and fastai correctly installed. By the way, I&#39;m using Google Colab here. If you have an own machine with a CUDA suitable GPU, you can also use that one of course. . ! pip install --upgrade pip ! pip uninstall torch torchvision fastai -y ! curl -s https://course.fast.ai/setup/colab | bash ! pip uninstall torch torchvision -y ! pip install torch==1.4.0 torchvision==0.5.0 . Okay, let&#39;s load the libraries. . %reload_ext autoreload %autoreload 2 %matplotlib inline import torch import torchvision import fastai import platform print(&#39;python version: {}&#39;.format(platform.python_version())) print(&#39;torch version: {}&#39;.format(torch.__version__)) print(&#39;torchvision version: {}&#39;.format(torchvision.__version__)) print(&#39;fastai version: {}&#39;.format(fastai.__version__)) print(&#39;cuda available: {}&#39;.format(torch.cuda.is_available())) print(&#39;num gpus: {}&#39;.format(torch.cuda.device_count())) print(&#39;gpu: {}&#39;.format(torch.cuda.get_device_name(0))) from fastai.vision import * from fastai.metrics import accuracy from torch.nn.functional import softmax . python version: 3.6.9 torch version: 1.4.0 torchvision version: 0.5.0 fastai version: 1.0.61 cuda available: True num gpus: 1 gpu: Tesla P100-PCIE-16GB . . Note: If you also use Google Colab, keep in my that the GPU could be different for you, since Google Colab assigns you a random GPU every time you start a new session. However, for our example here it actually doesn&#8217;t matter what kind of GPU from Google Colab we use. . Okay, let&#39;s implement our metric. The fastai documentation shows how this can be done. At first I was a bit confused but they actually describe it quite clearly. First of all, we need to think about whether our metric is an average over all the elements in our dataset. Is this the case for the log loss? Let&#39;s take a look at the log loss formula. . $$logloss = - frac{1}{N} sum_{i=1}^N{ sum_{j=1}^M}y_{ij}log(p_{ij})$$ . $N$ is the total number of samples (in our case images), $M$ is the number of categories, $log$ is the natural logarithm, $y_{ij}$ is an indicator functions that returns 1 if sample $i$ belongs to catgeory $j$ (0 otherwise) and $p_{ij}$ is the predicted probability that sample $i$ belongs to category $j$. As we can see, we sum over all samples $i$ and divide that sum by the total number of samples $N$. This means the log loss metric is indeed an average over all elements of our dataset. However, we usually don&#39;t show all samples of our dataset to our learner at once. We show them in batches instead. So, let&#39;s write a function that takes the final network predictions of each sample of a batch as well as their corresponding target categories as input and that outputs the log loss of that batch (average of the log losses of all sample of the batch). . def log_loss(preds: Tensor, targs: Tensor) -&gt; Rank0Tensor: &quot;Computes accuracy with `targs` when `preds` is bs * n_classes.&quot; epsilon = 1e-15 # calculate probs p = softmax(preds, dim=1) p = p / p.sum(dim=1).reshape(-1,1) # apply min max rule p = torch.clamp(p, min=epsilon) p = torch.clamp(p, max=(1-epsilon)) # calculate log probs p = torch.log(p) # convert targs as one-hot vectors n = len(targs) m = p.shape[1] zeros = torch.zeros(n, m) if preds.is_cuda: zeros = zeros.cuda() y = zeros.scatter(1, targs.unsqueeze(1),1.).float() # calculate and return log loss return (-1 / n) * (y * p).sum() . preds are the network predictions for each sample of the batch (tensor of size batch size x number of categories). targs are the target category labels for each sample of the batch (tensor of size batch size x 1). Since our network predictions preds are the predictions before applying the Softmax function to them (in PyTorch the Softmax is part of the CrossEntropy loss function as alluded here and here), we need to apply Softmax manually to our predictions preds to receive our probabilities $p_{ij}$. Furthermore, due to potential computational inaccuracies we normalize all probabilites for each sample again (i.e. we divide each probability of a category by the sum of the probabilities of all categories) to make sure they really add up to 1 (as alluded by Kaggle). I&#39;m actually not sure whether this is absolutely necessary, since I believe PyTorch&#39;s Softmax function should already take care of it, but it doesn&#39;t hurt either. Then, due to mathematical reasons we also need to apply the MinMax rule on our probabilities. . Now, we can use this function as metric for our learner. But why do we have to define our function only on a batch and not the whole dataset? Well, we only have to define it for the batch, because fastai applies our function to the whole dataset automatically. For every batch fastai calls our funcion and receives the log loss for that batch in return. As a result, if we have e.g. 50 batches, fastai generates 50 log loss values. One for each batch. Finally, fastai takes the average of these 50 log loss values and receives the final log loss value for the whole dataset. This makes sense, since the average of a bunch of averages is the overall average. . However, not every metric is an average over the whole dataset. This is actually the case for e.g. the Precision metric. Or we could even come up with totally different metrics like e.g. a metric that caluclates the maximal RAM usage during training. To implement metrics, that are not an average over all samples of a dataset, we need to use a callback. Check out the fastai documentation to find out how this can be implemented. Our custom log loss metric could be written using a callback as follows. . class LogLoss2(Callback): def on_epoch_begin(self, **kwargs): self.sum, self.total = 0., 0. def on_batch_end(self, last_output, last_target, **kwargs): epsilon = 1e-15 # calculate probs p = softmax(last_output, dim=1) p = p / p.sum(dim=1).reshape(-1,1) # apply min max rule p = torch.clamp(p, min=epsilon) p = torch.clamp(p, max=(1-epsilon)) # calculate log probs p = torch.log(p) # convert targs as one-hot vectors n = len(last_target) m = p.shape[1] zeros = torch.zeros(n, m) if last_output.is_cuda: zeros = zeros.cuda() y = zeros.scatter(1, last_target.unsqueeze(1),1.).float() # update sum and total count self.sum += -1 * (y * p).sum() self.total += n def on_epoch_end(self, last_metrics, **kwargs): return add_metrics(last_metrics, self.sum/self.total) . The function on_batch_end contains actually almost the same code as our metric function from above. However, now we have to take care ourselves that the final log loss is calulcated over all batches after the last epoch of training. We do this using the on_epoch_end method. . Okay, now let&#39;s test our custom log loss metric. Let&#39;s put our two versions of it in a list and let&#39;s also add the accuracy for completeness. . metrics = [log_loss, LogLoss2(), accuracy] . Let&#39;s use a sample of the MNIST dataset for testing. First, we need to download the dataset. . path = untar_data(URLs.MNIST_SAMPLE); path . PosixPath(&#39;/root/.fastai/data/mnist_sample&#39;) . Then, we load the dataset into a DataBunch object. . tfms = get_transforms(do_flip=False) data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26, num_workers=1, bs=64) . Let&#39;s look at a few samples. . data.show_batch(rows=3, figsize=(5,5)) . It only shows samples of the digit 7 and the digit 3, because the dataset we use here is just a subset of the real MNIST dataset. However, a subset is enough to have a first test of our custom metric. Now, let&#39;s create a learner object, add our metric to it and start training a model for two epochs. . learn = cnn_learner(data, models.resnet18, metrics=metrics) learn.fit(2) . epoch train_loss valid_loss log_loss log_loss2 accuracy time . 0 | 0.203924 | 0.107662 | 0.107662 | 0.107662 | 0.959764 | 00:19 | . 1 | 0.117651 | 0.054363 | 0.054363 | 0.054363 | 0.980864 | 00:19 | . What we can see is that our custom log loss metric log_loss as well as its callback version log_loss2 produced the same results as the loss on the validation set valid_loss after each epoch. This means that our implementations is probably correct. . Next, let&#39;s try a more complicated dataset and more training epochs. Let&#39;s choose the PETs dataset. First, we need to download it and load it into a DataBunch object as we did for MNIST. . path = untar_data(URLs.PETS) path_anno = path/&#39;annotations&#39; path_img = path/&#39;images&#39; fnames = get_image_files(path_img) pat = r&#39;/([^/]+)_ d+.jpg$&#39; data = ImageDataBunch.from_name_re( path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=64 ).normalize(imagenet_stats) . Let&#39;s look at a few samples. . data.show_batch(rows=3, figsize=(7,6)) . Now, let&#39;s create a learner object, add our metrics to it and start training a model for eight epochs. . learn = cnn_learner(data, models.resnet50, metrics=metrics) learn.fit_one_cycle(8, max_lr=2e-02) . epoch train_loss valid_loss log_loss log_loss2 accuracy time . 0 | 0.692790 | 0.730190 | 0.730190 | 0.730190 | 0.813261 | 01:21 | . 1 | 1.482390 | 4.922840 | 4.921004 | 4.921004 | 0.430311 | 01:19 | . 2 | 1.097990 | 1.269556 | 1.269556 | 1.269555 | 0.695535 | 01:20 | . 3 | 0.734669 | 0.812989 | 0.812989 | 0.812989 | 0.758457 | 01:20 | . 4 | 0.563721 | 0.491822 | 0.491822 | 0.491822 | 0.859946 | 01:19 | . 5 | 0.393704 | 0.355768 | 0.355768 | 0.355768 | 0.880920 | 01:20 | . 6 | 0.265045 | 0.235767 | 0.235767 | 0.235767 | 0.926928 | 01:19 | . 7 | 0.187495 | 0.227037 | 0.227037 | 0.227037 | 0.930988 | 01:19 | . As we can see, the loss on the validation set valid_loss matches our two versions of the log loss metric after almost every epoch exactly. Only after epoch 1 there is a small difference. However, the values still match roughly. I assume the difference is based on a slight variation of the implementation that calculates the valid_loss compared to our own implementation. However, I haven&#39;t been able two find out yet what this variation is. Nevertheless, since the difference is not huge, I believe my implementation of the log loss metric is correct. So, we are done! .",
            "url": "https://bam098.github.io/ai-blog/metric/fastai/image%20classification/cnn/2020/06/19/fastai-custom-metrics.html",
            "relUrl": "/metric/fastai/image%20classification/cnn/2020/06/19/fastai-custom-metrics.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating a Mushroom Image Dataset Using Google Images",
            "content": "Recently, a friend of mine told me that she would like to collect mushrooms in the wild. She grew up in a bigger city in Colombia and apparently collecting mushrooms was not a common thing to do there. Thus, when she visits me next time, she wants to try it out. However, she is also a bit afraid of picking up toxic mushrooms by accident. That&#39;s why I thought it might be quite handy to have a mushroom classifier. The classifier should take a photo of a mushroom and then tell me what type of mushroom the photo shows. So, it is basically an image classification problem. I already showed how to classify images of distracted drivers using the fastai library in another blog post. To classify photos of mushrooms we could try the same approach. However, for the distracted driver classification I was able to use a dataset from Kaggle to train the classification model. For mushrooms on the other hand I have not been able to find any ready-to-use image dataset. Hence, I need to create such a dataset by myself. How can this be done? . I found this project by three students from the University of Helsinki in which they also trained a model to classify mushroom photos. They collected the mushroom photos for their training set from the website mushroom.world using a web scraper. I could use the same way to collect the photos. However, I decided to keep looking for a more general approach. A more general approach would be handy, because if I need a classifier that classifies photos of e.g. birds next time, I can reuse that same approach. Adrian Rosenberg described such a general way to collect images in a great post in his blog. He simply used Google Images. Later, the team of fast.ai picked up and refined his idea for their Deep Learning course. In this blog post I want to show how to apply their method to collect mushroom photos for training a classifier using Google Colab. If you rather want to read the original posts, you can find the blog post from Adrian here and the Jupyter notebook from fast.ai here. . . Warning: The focus of this blog post is to describe how to create an own mushroom image dataset using Google Images. It is not meant to show how to create a ready-to-use mushroom classifier. Before such a classifier can be used in real life more types of mushrooms than I use here need to be included and way more testing has to be done. Furthermore, it is important to create the dataset using a sufficient amount of domain knowledge regarding mushrooms. Otherwise, the consequences could be severe. Eating toxic mushrooms is dangerous! Moreover, in general when creating a dataset there are also ethical considerations that should be paid attention to. You can find more information about that in this lecture of fastai&#8217;s Deep Learning course. . Since I want to use Google Colab for model training, I need to put the data to my Google Drive. Thus, Google Drive needs to be mounted. So, let&#39;s create a directory for our data on Google Drive, which I simply name mushroom-dataset here, and then let&#39;s mount Google Drive. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) root_dir = &#39;/content/gdrive/My Drive/&#39; base_dir = root_dir + &#39;fastai-v3/data/mushrooms-dataset/&#39; . If you have an own machine (remote or local), that has a CUDA suitable GPU, you can use this one instead of Google Colab of course. However, I chose to use Google Colab here, since it doesn&#39;t come with any costs (at least not in the free version and the free version is enough here). . Next, let&#39;s load some magics (you can read more about them in my earlier blog post). . %reload_ext autoreload %autoreload 2 %matplotlib inline . Let&#39;s also make sure we use the newest versions of PyTorch and the fastai library. In my experience Google Colab usually always has the newest versions of these libraries automatically installed. However, to be sure we could uninstall the currently installed versions and then use the setup script for Colab offered by fastai to install the newest ones. . ! pip uninstall torch torchvision fastai -y . ! curl -s https://course.fast.ai/setup/colab | bash . However, currently there is one more problem. The latest version of PyTorch (version 1.5) doesn&#39;t work together with the current version of fastai (version 1.0.61). The problem can be solved by downgrading PyTorch to version 1.4.0. . ! pip uninstall torch torchvision -y ! pip install torch==1.4.0 torchvision==0.5.0 . I&#39;m sure this problem will be fixed eventually. Thus, the downgrading probably won&#39;t be necessary in the future anymore. . Next, let&#39;s display the version of PyTorch, fastai and numpy. . import torch import torchvision import fastai import numpy . print(&#39;torch version: {}&#39;.format(torch.__version__)) print(&#39;torchvision version: {}&#39;.format(torchvision.__version__)) print(&#39;fastai version: {}&#39;.format(fastai.__version__)) print(&#39;numpy version: {}&#39;.format(numpy.__version__)) . torch version: 1.4.0 torchvision version: 0.5.0 fastai version: 1.0.61 numpy version: 1.18.3 . Let&#39;s also make sure that CUDA is available. . torch.cuda.is_available() . True . Now, let&#39;s load the functions that we need later. . from fastai.vision import * from fastai.metrics import accuracy import numpy as np from fastai.widgets import * . To obtain reproducible results we also need to run the following code. You can read more about it here. . np.random.seed(0) torch.manual_seed(0) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False . However, this will only make the training step reproducible. The dataset creation won&#39;t be reproducible, since we are going to use the Google Image Search, which most likely gives different results over time. . . Note: In a real project we should archive our created dataset. Furthermore, if the dataset is likely to change over time (e.g. through adding more data), we can also version it. One way to do that is by using DVC. I haven&#8217;t used DVC yet, but I might look into it in the future. . To create the dataset we need to run the following two steps: . We need to search for images we want to download using Google and save the URLs of these images. | Then, we need to download the images by their URLs. | . Creating a List of Image URLs . First of all, we need to decide which classes our dataset should have. From my point of view we have two options here: . We could use the two classes toxic and non-toxic. | We could use an own class for each type of mushroom. | . I decided to go for the second option, since I thought it might give better results when searching for images of these classes using Google. But how many mushroom types should I use? The more the better probably. However, since I just want to show the general approach here, I decided to simply include the following eight common mushroom types for now: . Amanita Muscaria | Boletus Santanas | Amanita Phalloides | Amanita Virosa | Agaricus Campestris | Morchella Esculenta | Cantharellus Cibarius | Boletus Edulis | . For each of these classes we need to go to Google Images (on our local machine!) and search for images of that class. The more specific the search query is the better will be the result and we need to do less cleaning of the data later. It might be a good idea to exclude a few terms in the search query. For instance, I noticed that when I searched for amanita muscaria, a photo of a music band also appeared in the search results. It turns out that this band has a song with that name. I want to exclude those kind of images of course. We can do that by adding e.g. -music to the search query resulting into amanita muscaria -music. Then, I looked through the search result again and noticed a few more weird images, which I also excluded in the same way. My final search queries for each class looked like the following. . Amanita Muscaria . &quot;amanita muscaria&quot; -facebook -twitter -youtube -slideshare -reddit -apotheke -researchgate -sciencedirect -shop -music -kunst -kunstsammlung -illustration -cartoon -tattoo -christmas -weihnachten -halloween -filz -hat -shirt -fandom -modell -stamp -map -getrocknet -cloning . Boletus Santanas . &quot;boletus satanas&quot; -facebook -youtube -amazon -soundcloud -spotify -shazam -bandcamp -necrocock -cremaster -researchgate -magazine -deviantart -comics -illustration -icon -reklamebilder -briefmarke -briefmarkenwelt -stamp -modell -shirt -map . Amanita Phalloides . &quot;amanita phalloides&quot; -facebook -twitter -youtube -amazon -slideshare -soundcloud -spotify -heavenchord -journals -czechmycology -researchgate -sciencedirect -semanticscholar -slideplayer -deviantart -illustration -alice -icon -3dcadbrowser -stamp -hosen -map -fliegenpilz -chemistry -chemical -mykothek . Amanita Virosa . &quot;amanita virosa&quot; -facebook -apple -twitter -youtube -amazon -bookdepository -animaltoyforum -slideplayer -review -table -soundcloud -spotify -shazam -recordshopx -album -deviantart -illustration -danbooru -character -stamp -map -microscope -chemistry -chemisch -chemical -fungalspores -asystole -mykothek . Agaricus Campestris . &quot;agaricus campestris&quot; -facebook -twitter -tumblr -youtube -amazon -reddit -researchgate -japanjournalofmedicine -semanticscholar -slideplayer -untersuchung -shop -gamepedia -illustration -zeichnung -drawing -model -modell -clipdealer -clipart -cyberleninka -scandposters -fototapete -philatelie -stamp -map -candy -pferd -buy -pilzkorb -pilzgericht -mykothek . Morchella Esculenta . &quot;morchella esculenta&quot; -facebook -twitter -youtube -reddit -amazon -alibaba -ebay -tripadvisor -researchgate -thesis -semanticscholar -sciencedirect -linkedin -dribbble -music -art -artsy -kunst -illustrated -clipartlogo -3D -plakate -stamp -monkstars -shirt -map -dried -porzellan -mykothek . Cantharellus Cibarius . &quot;cantharellus cibarius&quot; -facebook -twitter -youtube -amazon -tripadvisor -researchgate -medical -semanticscholar -sciencedirect -dribbble -art -deviantart -clipart -cubanfineart -hiclipart -modell -sketchup -fandom -stamp -briefmarken -map -getrocknete -food -shirt -gericht -basket -korb -dried -porzellan -mykothek . Boletus Edulis . &quot;boletus edulis&quot; -youtube -amazon -alibaba -ebay -tripadvisor -researchgate -semanticscholar -scientific -mikroskopie -russianpatents -onlineshop -illustration -modell -servietten -fandom -briefmarken -shirt -stamp -map -cutted -dried -getrocknet -salsa -sauce -samen -basket -korb . . . Important: The following steps for receiving the image URLs need to be done for each of the eight mushroom classes. . After you typed in the search query and pressed the search button, scroll down the search results until there are either no more images or the button Show more results appears. If the button Show more results appears and you still need more images, you need to click the button and keep scrolling until there are no more images. Google Images shows 700 images at maximum. . . Note: There is a handy tool called gi2ds (see also on GitHub) written by Christoffer Björkskog that let you directly select which images of the search result you want to take and which image you do not want to take. However, Google now blocked loading jQuery.min.js which is required by gi2ds. Unfortunately, I don&#8217;t have too much knowledge about JavaScript yet to fix the problem by myself. Thus, I cannot use the tool now. However, you still should check the tool out at a later time, since the issue might be fixed in the future. . Now, we need to get the URL of each image of the search result. We can use JavaScript for this. Therefor, we need to open the JavaScript console in the browser. For instance, in Google Chrome we can do this by pressing Ctrl+Shift+j(Windows/Linux) or Cmd+Opt+j(MacOS). In Firefox we need to press Ctrl+Shift+k (Windows/Linux) or Cmd+Opt+k (MacOS) instead. When the JavaScript console is open, we need to copy the following code and paste it into the console. Make sure, if you have any ad blocking software installed in your browser (e.g. uBlock, AdBlock, AdBlockPlus), that you disable it before running the JavaScript code in the console (otherwise the window.open() command won&#39;t work). Then, press Enter. . urls=Array.from(document.querySelectorAll(&#39;.rg_i&#39;)).map( el=&gt; el.hasAttribute(&#39;data-src&#39;)?el.getAttribute(&#39;data-src&#39;):el.getAttribute(&#39;data-iurl&#39;) ); window.open(&#39;data:text/csv;charset=utf-8,&#39; + escape(urls.join(&#39; n&#39;))); . . After running the code a file called Download.csv should get downloaded. This file contains the URLs of the images of the mushroom class you were searching for on Google. Let&#39;s rename the file to the corresponding class name. For instance, if I searched for images of the amanita muscaria, I will rename the file from Download.csv to amanita_muscaria.csv. You should end up with eight of these CSV files. One for each class. Finally, upload them to Google Drive. I chose to put them in a directory called urls, that I created in my base_dir directory (see above). . Download Images . . Important: After uploading the CSV files (containing the image URLs) to Google Drive we need to switch from our local machine to Google Colab, since we want to train our classification model there. . Now on Google Colab, we need to download the images using their URLs, that are stored in the eight CSV files. To do this we need to run the following steps for each mushroom class: . Create a directory that is named according to the class. | Take the corresponding CSV file with the image URLs and download each image by those URLs. | Save the downloaded images into the created directory. | . We should end up with eight directories. One for each mushroom class containing the corresponding images. . How can we do this in code? Fastai actually already provides a function called download_images, that takes a CSV file with image URLs and downloads the images by these URLs to a specified location. However, only images that can be opened are downloaded. To be able to execute download_images for each of the eight CSV files I wrote an own little wrapper function. This wrapper function takes a dictionary, which stores the class names as keys and the paths to the CSV files as values, and then loops over this dictionary to call download_images for each entry of the dictionary (i.e. each class). . def download_dataset(category_urls: Dict[str, Path], out_dir: Path, max_pics: int) -&gt; None: categories = category_urls.keys() for c in categories: print(&#39;downloading images of class {}&#39;.format(c)) dest = out_dir/c url_file_path = category_urls[c] download_images(url_file_path, dest, max_pics=max_pics) . The dictionary with the class names as keys and the CSV file paths as values looks like the following. . category_urls = { &#39;agaricus_campestris&#39;: base_dir + &#39;urls/agaricus_campestris.csv&#39;, &#39;amanita_muscaria&#39;: base_dir + &#39;urls/amanita_muscaria.csv&#39;, &#39;amanita_phalloides&#39;: base_dir + &#39;urls/amanita_phalloides.csv&#39;, &#39;amanita_virosa&#39;: base_dir + &#39;urls/amanita_virosa.csv&#39;, &#39;boletus_edulis&#39;: base_dir + &#39;urls/boletus_edulis.csv&#39;, &#39;boletus_satanas&#39;: base_dir + &#39;urls/boletus_satanas.csv&#39;, &#39;cantharellus_cibarius&#39;: base_dir + &#39;urls/cantharellus_cibarius.csv&#39;, &#39;morchella_esculenta&#39;: base_dir + &#39;urls/morchella_esculenta.csv&#39;, } category_urls . {&#39;agaricus_campestris&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/agaricus_campestris.csv&#39;, &#39;amanita_muscaria&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/amanita_muscaria.csv&#39;, &#39;amanita_phalloides&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/amanita_phalloides.csv&#39;, &#39;amanita_virosa&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/amanita_virosa.csv&#39;, &#39;boletus_edulis&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/boletus_edulis.csv&#39;, &#39;boletus_satanas&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/boletus_satanas.csv&#39;, &#39;cantharellus_cibarius&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/cantharellus_cibarius.csv&#39;, &#39;morchella_esculenta&#39;: &#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/urls/morchella_esculenta.csv&#39;} . Let&#39;s also store a list of the classes. To create such a list we simply need to take all the keys from our dictionary. . classes = list(category_urls.keys()); classes . [&#39;agaricus_campestris&#39;, &#39;amanita_muscaria&#39;, &#39;amanita_phalloides&#39;, &#39;amanita_virosa&#39;, &#39;boletus_edulis&#39;, &#39;boletus_satanas&#39;, &#39;cantharellus_cibarius&#39;, &#39;morchella_esculenta&#39;] . Now let&#39;s specify where the directories for each class with the corresponding images should be stored. I decided to store them in a directory called data that I also created in the base_dir directory. . out_dir = Path(base_dir + &#39;data&#39;); out_dir . PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data&#39;) . Finally, we can call our wrapper function to download the images of the eight mushroom classes. Since I want to get as many images as possible, I specified to download maximally 700 images. We actually can&#39;t have more than 700 image URLs in each CSV file, since Google Images only gives us 700 at maximum. . download_dataset(category_urls, out_dir, 700) . downloading images of class agaricus_campestris downloading images of class amanita_muscaria downloading images of class amanita_phalloides downloading images of class amanita_virosa downloading images of class boletus_edulis downloading images of class boletus_satanas downloading images of class cantharellus_cibarius downloading images of class morchella_esculenta . Okay, let&#39;s check if the directories were created. . out_dir.ls() . [PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/agaricus_campestris&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/amanita_muscaria&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/amanita_phalloides&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/amanita_virosa&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/boletus_edulis&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/boletus_satanas&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/cantharellus_cibarius&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/mushrooms-dataset/data/morchella_esculenta&#39;)] . This looks good. Now, let&#39;s go through all directories and check if some of the images are corrupted (e.g. since they couldn&#39;t be downloaded properly) or if they are okay and can be opened. If they are corrupted, we will remove them. Again, we don&#39;t need to code this all by ourselves. Fastai offers a function verify_images that checks all images of the directory and removes corrupted images. I just wrote a simply wrapper function again, that runs verify_images for all of our eight directories. . def remove_unaccessible_images(categories: List[str], ds_dir: Path) -&gt; None: for c in categories: print(&#39;removing unaccessible images of class {}&#39;.format(c)) verify_images(ds_dir/c, delete=True, max_size=500) print() . Now, let&#39;s call the wrapper function. . remove_unaccessible_images(classes, out_dir) . removing unaccessible images of class agaricus_campestris removing unaccessible images of class amanita_muscaria removing unaccessible images of class amanita_phalloides removing unaccessible images of class amanita_virosa removing unaccessible images of class boletus_edulis removing unaccessible images of class boletus_satanas removing unaccessible images of class cantharellus_cibarius removing unaccessible images of class morchella_esculenta . According to the output there were no corrupted images found. If there have been any, we would have seen the file paths of them in the output. Next, let&#39;s load our dataset into an ImageDataBunch object. Since our dataset is structured among separate directories for the eight classes, we can use the from_folder method here. We split the dataset into 80% training and 20% validation set. Furthermore, we apply standard data augmentation and a normalization using ImageNet statistics to the data. This is similar to what I did when I trained a model to classify images of distracted drivers. For more information regarding how to load the data as an ImageDataBunch see my earlier blog post. . data = ImageDataBunch.from_folder( out_dir, train=&quot;.&quot;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4 ).normalize(imagenet_stats) . Let&#39;s check how many classes were loaded and how big our training and validation set is. . print(&#39;num classes: {}&#39;.format(data.c)) print(&#39;train_ds size: {}&#39;.format(len(data.train_ds))) print(&#39;valid_ds size: {}&#39;.format(len(data.valid_ds))) . num classes: 8 train_ds size: 3276 valid_ds size: 818 . Good! All eight classes were loaded. Let&#39;s also display their class names. . data.classes . [&#39;agaricus_campestris&#39;, &#39;amanita_muscaria&#39;, &#39;amanita_phalloides&#39;, &#39;amanita_virosa&#39;, &#39;boletus_edulis&#39;, &#39;boletus_satanas&#39;, &#39;cantharellus_cibarius&#39;, &#39;morchella_esculenta&#39;] . Finally, let&#39;s also look at a few images of our dataset. . data.show_batch(rows=3, figsize=(7,8)) . Now, we are ready to train an initial model. . Train Initial Model . To train a mushroom classification model using our created dataset I am going to follow the same approach that I also used to train the model to classify images of distracted drivers. Thus, I will only briefly describe the training process here. Again, for more details check out my earlier blog post. . First, let&#39;s create a cnn_learner object with a ResNet50 network model architecture. . learn = cnn_learner(data, models.resnet50, metrics=accuracy) . We are going to use the 1-cycle-policy to train our model. Therefor, we need a maximum value for the learning rate hyper-parameter. To find a good value for the learning rate we can use the learning rate finder lr_find. . learn.lr_find() learn.recorder.plot() . The minimum of the graph is at 1e-01. We should choose a number 10 times smaller than this minimum to set the learning rate. However, through some experiments I found out that 1e-03 works even a bit better here. Using this learning rate value let&#39;s train the last layer of the network for 6 epochs now. . learn.fit_one_cycle(6, max_lr=1e-03) . epoch train_loss valid_loss accuracy time . 0 | 1.617568 | 0.832265 | 0.768949 | 01:02 | . 1 | 1.089216 | 0.789734 | 0.788509 | 01:02 | . 2 | 0.790581 | 0.689489 | 0.821516 | 01:02 | . 3 | 0.583131 | 0.610158 | 0.828851 | 01:02 | . 4 | 0.432905 | 0.604717 | 0.833741 | 01:02 | . 5 | 0.353715 | 0.605169 | 0.837408 | 01:02 | . We reached an accuracy of 83.74%. This is not too bad. Let&#39;s save our model. . learn.save(&#39;stage-1&#39;) . Then, let&#39;s unfreeze all the layers and train the whole network for 12 more epochs. For the deeper layers I keep using the learning rate value that we found by using the learning rate finder. For the earlier layers we should use a value ten times smaller than that for the learning rate. . learn.unfreeze() learn.fit_one_cycle(12, max_lr=slice(1e-4,1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.358885 | 0.609037 | 0.828851 | 01:21 | . 1 | 0.383087 | 0.813287 | 0.790954 | 01:21 | . 2 | 0.460756 | 0.901529 | 0.803178 | 01:21 | . 3 | 0.488975 | 0.646106 | 0.815403 | 01:21 | . 4 | 0.393394 | 0.593758 | 0.839853 | 01:21 | . 5 | 0.303394 | 0.739284 | 0.812958 | 01:20 | . 6 | 0.253918 | 0.650706 | 0.847188 | 01:21 | . 7 | 0.192970 | 0.617252 | 0.847188 | 01:21 | . 8 | 0.124574 | 0.611874 | 0.853301 | 01:21 | . 9 | 0.085323 | 0.632054 | 0.864303 | 01:21 | . 10 | 0.051538 | 0.625571 | 0.865526 | 01:20 | . 11 | 0.043466 | 0.614751 | 0.870416 | 01:20 | . We could improve our model up to 87.04% accuracy. Let&#39;s save our model. . learn.save(&#39;stage-2&#39;) . Next, let&#39;s see which mistakes our model makes. We can use the ClassificationInterpretation class for this. It will give us the samples of the validation set with the highest losses. . interp = ClassificationInterpretation.from_learner(learn) losses, idxs = interp.top_losses() . Let&#39;s plot the images with the highest losses. . interp.plot_top_losses(9, figsize=(15,11)) . Okay, as we can see we have some problems with our dataset. We have some weird images like e.g. the first image in the second row, which shouldn&#39;t be in our dataset. Furthermore, we also have images with incorrect labels like e.g. the first image in the first row. It&#39;s clearly an Amanita Muscaria and our model also recognized that correctly. However, that image is labeled incorrectly as Amanita Virosa. Well, that we don&#39;t get a clean, ready-to-use image dataset using Google Images is actually not surprising. So, we need to clean the data! However, before we do that let&#39;s also plot the confusion matrix for completeness. . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Additionally, let&#39;s also plot the classes our model gets most confused about. . interp.most_confused(min_val=2) . [(&#39;amanita_virosa&#39;, &#39;agaricus_campestris&#39;, 7), (&#39;amanita_virosa&#39;, &#39;amanita_muscaria&#39;, 7), (&#39;agaricus_campestris&#39;, &#39;amanita_phalloides&#39;, 6), (&#39;agaricus_campestris&#39;, &#39;amanita_virosa&#39;, 6), (&#39;amanita_phalloides&#39;, &#39;amanita_virosa&#39;, 6), (&#39;agaricus_campestris&#39;, &#39;boletus_edulis&#39;, 5), (&#39;amanita_phalloides&#39;, &#39;boletus_edulis&#39;, 5), (&#39;amanita_virosa&#39;, &#39;amanita_phalloides&#39;, 5), (&#39;boletus_edulis&#39;, &#39;agaricus_campestris&#39;, 4), (&#39;amanita_phalloides&#39;, &#39;agaricus_campestris&#39;, 3), (&#39;amanita_phalloides&#39;, &#39;boletus_satanas&#39;, 3), (&#39;amanita_virosa&#39;, &#39;boletus_satanas&#39;, 3), (&#39;boletus_edulis&#39;, &#39;boletus_satanas&#39;, 3), (&#39;boletus_satanas&#39;, &#39;amanita_muscaria&#39;, 3), (&#39;boletus_satanas&#39;, &#39;boletus_edulis&#39;, 3), (&#39;morchella_esculenta&#39;, &#39;cantharellus_cibarius&#39;, 3), (&#39;agaricus_campestris&#39;, &#39;boletus_satanas&#39;, 2), (&#39;agaricus_campestris&#39;, &#39;cantharellus_cibarius&#39;, 2), (&#39;amanita_muscaria&#39;, &#39;boletus_satanas&#39;, 2), (&#39;amanita_phalloides&#39;, &#39;cantharellus_cibarius&#39;, 2), (&#39;amanita_phalloides&#39;, &#39;morchella_esculenta&#39;, 2), (&#39;boletus_satanas&#39;, &#39;amanita_phalloides&#39;, 2), (&#39;boletus_satanas&#39;, &#39;amanita_virosa&#39;, 2), (&#39;cantharellus_cibarius&#39;, &#39;agaricus_campestris&#39;, 2), (&#39;cantharellus_cibarius&#39;, &#39;amanita_phalloides&#39;, 2), (&#39;morchella_esculenta&#39;, &#39;agaricus_campestris&#39;, 2), (&#39;morchella_esculenta&#39;, &#39;amanita_phalloides&#39;, 2), (&#39;morchella_esculenta&#39;, &#39;boletus_satanas&#39;, 2)] . Now, let&#39;s clean our dataset. . Clean Dataset . To clean the dataset we can use a widget provided by fastai that is called ImageCleaner. This widget will make the cleaning process a lot easier. . . Note: (Update) In an earlier version of this blog post I mentioned that the widget is not able to run on Google Colab. Hence, we need to do the cleaning in a Jupyter notebook on our local machine. Fortunately, this is no longer the case. The widget does run in Google Colab now. Thanks to Elie GAKUBA for pointing that out. . For cleaning the data we don&#39;t need to have it split into a training and validation set. Moreover, we also don&#39;t want to normalize the data, since we want to look at the original unchanged images. We want to load and then look at them to be able to decide whether we want to remove an image from the dataset or if we need to change an image&#39;s label. So, let&#39;s create a new ImageDataBunch object without the splitting and normalization. . data_bunch = ( ImageList.from_folder(out_dir) .split_none() .label_from_folder() .transform(get_transforms(), size=224) .databunch() ) . Now, we are ready to clean the data. As I mentioned before our trained model will help us cleaning the dataset. So, let&#39;s create a new cnn_learner and load our model. However, this time we are not going to use it for model training. . learn_cln = cnn_learner(data_bunch, models.resnet50, metrics=accuracy) learn_cln.load(&#39;stage-2&#39;); . So, why is our model useful for cleaning the data? Well, we can use our model to find the images for which our model gives high losses. A high loss means either a) our model is not very good or b) something is wrong with the image (e.g. weird image, wrong image label). Thus, let&#39;s check the images with a high loss. To obtain these images we can use the from_toplosses method of the DatasetFormatter class. . ds, idxs = DatasetFormatter().from_toplosses(learn_cln) . Now, we can use the ImageCleaner widget on the images with high losses. After executing the following command a graphical menu like the one below will appear. It shows four images of our dataset that resulted in a high loss. Below each image is a button Delete. By clicking on that button we can remove that image from the dataset. Moreover, there is also a drop down list containing the mushroom labels below each image, which makes it possible to change the label of that image. When we are done with these four images, we can click Next Batch to receive the next four images, which we can check as well. We can repeat this process until there are no more images left. In this case the message No images to show :) will appear. . ImageCleaner(ds, idxs, out_dir) . &#39;No images to show :)&#39; . . . Important: To avoid that our session on Google Colab runs out while using the widget we need to run some other cell periodically during the cleaning process (e.g. every five minutes). Thanks to Elie GAKUBA who pointed that out as well. . Besides images that don&#39;t show mushrooms we also don&#39;t want to have duplicate images in our dataset. The DatasetFormatter also has a method from_similar to give us similar images of our dataset indicating that these images could be duplicates. The similarity scores are computed from the layer activations of our network. . ds, idxs = DatasetFormatter().from_similars(learn_cln) . Getting activations... Computing similarities... . Now we can use the ImageCleaner widget again. This time it will show a graphical menu like the one below that always shows us a pair of images. Again, by clicking the Delete button we can remove an image from the dataset. After we decided whether we want to delete one of the two images (i.e. in case of duplicate images) or not we can click on Next Batch and the widget will show us the next pair of images. This process repeats until there are no more images. In this case the message No images to show :) will appear. . ImageCleaner(ds, idxs, out_dir, duplicates=True) . &#39;No images to show :). 285 pairs were skipped since at least one of the images was deleted by the user.&#39; . . . Important: Same as before, to avoid that our session on Google Colab runs out while using the widget we need to run another cell periodically during the cleaning process. . However, the images we decided to remove from the dataset are actually not deleted from our hard drive. Instead, the widget creates a CSV file called cleaned.csv, which only contains the images we didn&#39;t want to have removed from the dataset as well as their corrected labels. . By the way, the cleaning process can actually take quite some time. If you decide to take a break in between, you can simply use the current cleaned.csv file to create a new ImageDataBunch object using the from_csv method when you want to continue cleaning (instead of the from_folder method that we used before). . data_bunch = ( ImageList.from_csv(out_dir, &#39;cleaned.csv&#39;) .split_none() .label_from_folder() .transform(get_transforms(), size=224) .databunch() ) . When we finished the cleaning, we can train a new model with out cleaned dataset. . Continue Training . Let&#39;s use the cleaned.csv CSV file to load the dataset. So, let&#39;s create an ImageDataBunch object from that CSV file using the method from_csv. We split the dataset into 80% training and 20% validation set again. Moreover, we also apply basic data augmentation as well as normalization using the ImageNet statistics to the images of our dataset as before. . data = ImageDataBunch.from_csv( ds_dir, folder=&quot;.&quot;, valid_pct=0.2, csv_labels=&#39;cleaned.csv&#39;, ds_tfms=get_transforms(), size=224, num_workers=4 ).normalize(imagenet_stats) . Now, let&#39;s check how big our training and validation set is. . print(&#39;num classes: {}&#39;.format(data.c)) print(&#39;train_ds size: {}&#39;.format(len(data.train_ds))) print(&#39;valid_ds size: {}&#39;.format(len(data.valid_ds))) . num classes: 8 train_ds size: 1724 valid_ds size: 430 . It got a lot smaller! Well, I needed to remove a lot of images from the dataset. But let&#39;s check if we still have images of all eight classes. . data.classes . [&#39;agaricus_campestris&#39;, &#39;amanita_muscaria&#39;, &#39;amanita_phalloides&#39;, &#39;amanita_virosa&#39;, &#39;boletus_edulis&#39;, &#39;boletus_satanas&#39;, &#39;cantharellus_cibarius&#39;, &#39;morchella_esculenta&#39;] . Okay, we do have images of all eight classes. Let&#39;s also look at a few images of the dataset. . data.show_batch(rows=3, figsize=(7,8)) . We can&#39;t see any strange images, which is a good sign. However, these are only nine images of our dataset. Let&#39;s train the model to see whether we can train a better model using our cleaned data this time. First, let&#39;s create a cnn_learner with a ResNet50 network architecture again. . learn = cnn_learner(data, models.resnet50, metrics=accuracy) . Then, let&#39;s find the learning rate for the 1-cycle-policy. . learn.lr_find() learn.recorder.plot() . The value 1e-03 seems still to be a good value for the learning rate here. So, let&#39;s train the last network layer with that learning rate value for six epochs. . learn.fit_one_cycle(6, max_lr=1e-03) . epoch train_loss valid_loss accuracy time . 0 | 1.668653 | 0.290320 | 0.930233 | 00:23 | . 1 | 0.845852 | 0.196863 | 0.965116 | 00:22 | . 2 | 0.509902 | 0.162525 | 0.965116 | 00:22 | . 3 | 0.334273 | 0.130670 | 0.962791 | 00:22 | . 4 | 0.222514 | 0.130108 | 0.969767 | 00:22 | . 5 | 0.162503 | 0.129676 | 0.969767 | 00:22 | . Oh! We could improve a lot compared to our initial model. Let&#39;s save the current model. . learn.save(&#39;stage-3&#39;) . Now, we should unfreeze our model and train for 12 more epochs. . learn.unfreeze() learn.fit_one_cycle(12, max_lr=slice(1e-4,1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.099722 | 0.155721 | 0.960465 | 00:28 | . 1 | 0.110477 | 0.213842 | 0.953488 | 00:27 | . 2 | 0.180419 | 0.344579 | 0.930233 | 00:27 | . 3 | 0.190187 | 0.154539 | 0.960465 | 00:27 | . 4 | 0.166473 | 0.204827 | 0.953488 | 00:27 | . 5 | 0.142693 | 0.100933 | 0.976744 | 00:27 | . 6 | 0.101313 | 0.135420 | 0.969767 | 00:27 | . 7 | 0.071307 | 0.107984 | 0.976744 | 00:27 | . 8 | 0.052224 | 0.087856 | 0.979070 | 00:27 | . 9 | 0.039046 | 0.084210 | 0.979070 | 00:27 | . 10 | 0.029161 | 0.082786 | 0.981395 | 00:26 | . 11 | 0.025028 | 0.081143 | 0.981395 | 00:26 | . Okay. We have been able to improve even a bit further to an accuracy of about 98.14%. Let&#39;s safe the model! . learn.save(&#39;stage-4&#39;) . Although our new model was able to reach over 98% accuracy, we should take a look into where our model makes mistakes. . interp = ClassificationInterpretation.from_learner(learn) losses, idxs = interp.top_losses() . interp.plot_top_losses(9, figsize=(15,11)) . As we can see the first two images of the first row still have a wrong label. I must have missed them when I cleaned the dataset. The other mistakes are not very surprising, since our model mixes up classes of mushrooms that a very similar (e.g. Agaricus Campestris and Boletus Edulis). Let&#39;s also look at the confusion matrix. . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=2) . [(&#39;amanita_virosa&#39;, &#39;amanita_muscaria&#39;, 2), (&#39;boletus_edulis&#39;, &#39;agaricus_campestris&#39;, 2)] . This looks quite good actually. We trained a model that is able to classify images of mushrooms with an accuracy of over 98% on the validation set using an own image dataset for model training. . . Note: I assume we are able to reach such a high accuracy of over 98% with such a simple approach since we use a pretrained model based on the ImageNet dataset for model training (by using the standard configuration of the cnn_learner). This pretrained model was originally trained on 1000 ImageNet classes including a general mushroom class. Although it was not trained to distinguish different mushroom types, it probably at least knew something about mushrooms already. . However, although our model reaches an accuracy of over 98%, our model needs to tested on way more data before it can be used in practice. Furthermore, it would be also important that our classifier let&#39;s us know if it doesn&#39;t know a mushroom we show it. I only included eight types of mushrooms for our model, but there exist way more of course. Currently, our classifier would simply predict any of the eight classes when it sees an unknown mushroom type. So, be cautious. Mixing up mushroom can be dangerous! However, since the focus of this blog post is on creating an own training dataset and not on how to create a ready-to-use mushroom classifier, we are done for now. .",
            "url": "https://bam098.github.io/ai-blog/google%20images/dataset/image%20classification/fastai/cnn/google%20colab/2020/05/08/creating-mushroom-dataset.html",
            "relUrl": "/google%20images/dataset/image%20classification/fastai/cnn/google%20colab/2020/05/08/creating-mushroom-dataset.html",
            "date": " • May 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Format Portable Drives as exFat to use them under Linux, Windows and MacOS",
            "content": "Currently, I mainly use Ubuntu and MacOS. Furthermore, I also get in contact with Windows from time to time. To transfer data between machines running these different operating systems I use a USB flash drive. Unfortunately, many USB flash drives don&#39;t work under Linux, Windows and MacOS at the same time out of the box. For instance, a while ago I experienced that my USB flash drive works normally under Linux and Windows, but under MacOS I only had read access but no write access. This behavior is caused by the fact that many USB flash drives are formatted using the NTFS file system. . . Note: A file system is responsible for organizing a drive. For instance, it specifies how the data is stored or how file permission is handled. . NTFS is the file system Windows likes to use by default nowadays. However, NTFS doesn&#39;t work properly under MacOS. There are a few alternatives to NTFS. You can read more about them here. I was looking for a file system that works under Linux, Windows and MacOS. The exFat file system meets this requirement. In the following I want to describe how I formatted my USB flash drive as exFat on Ubuntu 18.04 LTS. To do so I actually followed a great tutorial written by Kevin Arrows. In the following I will summarize it in my own words. However, if you prefer to read the original tutorial, you can find it here. . . Note: If you want to format your USB flash drive as exFat under Windows, you can check out this tutorial. If you want to do it under MacOS, you can have a look here. However, I can&#8217;t guarantee that these tutorials actually work, since I haven&#8217;t tested them myself. . . Warning: If you have additional requirements besides that the USB flash drive should work under Linux, Windows and MacOS, you might want to consider other file systems as well. You can find additional information about which file system to use here and here. . . Important: (Update) Below I describe a way how to format a portable drive as exFat that was supposed to work under Linux, Windows and MacOS. Unfortunately, after I posted the first verison of this blog post I found out that it actually only works on Linux and MacOS but not Windows. That&#8217;s why I added an additional section ExFat on Windows (see below) later, that describes a way to format the drive as exFat which also works under Windows. If this is what you are looking for, you may want to scroll down to this section already and skip my first solution that only works on Linux and MacOS. If you only want to use the drive on Linux and MacOS on the other hand or if you want to get some additional information, you can keep reading from here. . First of all, we need to install the exFat drivers on Ubuntu. . sudo apt-get install exfat-fuse exfat-utils . Then, we can plug in the USB flash drive into our machine. It should be automatically recognized and mounted by Ubuntu. You should see an icon on your desktop that represents your USB flash drive. Alternatively, you should also be able to find your mounted USB flash drive under /media using your terminal (other Linux distributions usually mount it under /mnt). . Next, we need to find out the name of our drive. We can use fdisk for this. . sudo fdisk -l . The command above should print a list of devices that are attached to our system. Pay very close attention to find the right drive name, since we are going to erase all data of that drive later. . . Warning: Pay very close attention to find the right drive name, since we are going to erase all data stored on the drive! For instance, if you find a drive named /dev/sda (or something similar) in the device list, you must not choose this one! The drive /dev/sda is usually the drive that is used to boot GNU/Linux from. It is not your USB flash drive. You don&#8217;t want to format it! . Often times our USB flash drive will show up as /dev/sdb in the device list. When I ran fdisk -l, the /dev/sdb entry showed up like this in the device list. . Disk /dev/sdb: 14,5 GiB, 15518924800 bytes, 30310400 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x7dffaefd Device Boot Start End Sectors Size Id Type /dev/sdb1 2048 30310399 30308352 14,5G 86 NTFS volume set . You can also check if the total memory that is displayed for /dev/sdb matches your USB flash drive&#39;s memory. On my USB flash drive it is written that it has 16 GB of storage. In the device list /dev/sdb shows 14.5 GB which is not exactly 16 GB, but it comes closest among all devices in the device list. Hence, /dev/sdb must be our USB flash drive (here you can read more about why it doesn&#39;t show up as exactly 16 GB). But wait! We actually see /dev/sdb and /dev/sdb1 here. What do both mean? Well, /dev/sdb is the name of our drive, whereas /dev/sdb1 is the name of the partition on our drive. . . Note: A drive is usually divided into multiple sections, which we call partitions. This way we can separate the files that are stored on the drive in a certain way. For instance, usually the internal disk drive of our machine holds all operating system related files in one partition and all data files (e.g. photos, documents) in another partition. This often makes sense, because then we are able to format the partition holding our operating system to replace it with a newer one while keeping our data on the other partition untouched. However, in contrast to our internal disk drive our USB flash drive can often be simply organized as one big partition, since we only store our personal data there. When we run sudo fdisk -l on Ubuntu, all devices attached to our machine get listed. Usually our USB flash drive has the name /dev/sdb here. Each of its partitions is listed with a number attached to that name, e.g. /dev/sdb1, /dev/sdb2 etc. If our USB flash drive has just one partition, we will only find /dev/sdb1 in the device list. . In the following I&#39;m going to assume that /dev/sdb is the correct name of our drive. However, you should still double-check, if this is also the correct name of your drive on your system! Furthermore, make sure there isn&#39;t any data on the drive that you still need, since we are going to format the drive now. . . Warning: Make sure that you don&#8217;t have any data on the USB flash drive anymore that you still need. If there is still data on the drive that you still need, you have to make a backup of the data before you proceed. . Next, we want to delete the current filesystem of our USB flash drive to start with a fresh file system. To do that we need to unmount our drive first. . sudo umount /dev/sdb* . We should either get no output or the following output. No error should appear. . umount: /dev/sdb: not mounted. umount: /dev/sdb1: not mounted. . The icon on the desktop representing our USB flash drive should disappear and also under /media it shouldn&#39;t be mounted anymore (or /mnt on other Linux distributions). Now we can delete the file system. To do this we can use a tool called wipefs. . sudo wipefs -a /dev/sdb . We should get the following (or similar) output. . /dev/sdb: 2 bytes were erased at offset 0x000001fe (dos): 55 aa /dev/sdb: calling ioctl to re-read partition table: Success . . Note: wipefs actually doesn&#8217;t remove the filesystem itself or any other data on the drive but the filesystem&#8217;s signature. However, this is enough to be able to set up a new file system on the drive. You can read more about wipefs here. If you also want to immediately erase all data on your drive, check out this article. . Next, we need to create a new partition table. We can use fdisk for this. . sudo fdisk /dev/sdb . . Note: A partition table describes all partitions of the drive. . We should get the following output. . Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. Created a new DOS disklabel with disk identifier 0xa8df2b81. Command (m for help): . As we can see, a command prompt appeared. We need to type n and then Enter to create our partition. Then, we are asked to specify a few parameters that are needed to create the partition on our drive: a) what partition type we want, b) how many partitions should be created and c) from which to which sector on the drive should each partition go. Since we simply want a single partition on our USB flash drive that encompasses the whole drive space, we can use the default values that are provided to us here. . Welcome to fdisk (util-linux 2.31.1). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Device does not contain a recognized partition table. Created a new DOS disklabel with disk identifier 0xa8df2b81. Command (m for help): n Partition type p primary (0 primary, 0 extended, 4 free) e extended (container for logical partitions) Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-30310399, default 2048): 2048 Last sector, +sectors or +size{K,M,G,T,P} (2048-30310399, default 30310399): 30310399 Created a new partition 1 of type &#39;Linux&#39; and of size 14,5 GiB. Command (m for help): . However, as you can see there is still one problem here. The type of the created partition is Linux, which will make it difficult to use our USB flash drive on Windows and MacOS. We need to change the type! We can change the partition type with the fdisk command t. After hitting Enter we are asked to provide the hex code of the desired partition type. We can list all available types by typing L. In the upcoming list we should look for HPFS/NTFS/exFAT and take its hex code. Usually it will have the number 7. So, let&#39;s type 7 to change the type of our partition from Linux to HPFS/NTFS/exFAT. . Command (m for help): t Selected partition 1 Hex code (type L to list all codes): 7 Changed type of partition &#39;Linux&#39; to &#39;HPFS/NTFS/exFAT&#39;. Command (m for help): . . Important: Sometimes fdisk also asks you to remove a signature from the drive. Unfortunately, I haven&#8217;t really understood yet what this signature is used for. Is it the file system&#8217;s signature that should already been removed by wipefs? Well, I simply agreed to remove this signature as well. My USB flash drive still worked as expected after formatting. However, if your USB flash drive is extremely important to you, you might want to further investigate about this signature before you proceed. . Finally, we can execute all of our commands by typing w and Enter. Depending on how large your drive is, it might take a few seconds. We should see the following output. . Command (m for help): w The partition table has been altered. Calling ioctl() to re-read partition table. Syncing disks. . After creating the partition and the partition table, we can finally create the file system. We can do this with the following command. . sudo mkfs.exfat -n bam098 /dev/sdb1 . bam098 is just the name that I decided to give my USB flash drive. You can use any name you like here. After running the command we should see the following or a similar output (the mkexfatfs version might differ on your system). . mkexfatfs 1.2.8 Creating... done. Flushing... done. File system created successfully. . To check if there are any errors we can also run the following. . sudo fsck.exfat /dev/sdb1 . We should see the following or a similar output. . exfatfsck 1.2.8 Checking file system on /dev/sdb1. File system version 1.0 Sector size 512 bytes Cluster size 32 KB Volume size 14 GB Used space 2 MB Available space 14 GB Totally 0 directories and 0 files. File system checking finished. No errors found. . We should make sure that it says No errors found. Finally, I tested my USB flash drive, formatted with exFat as described above, on my Ubuntu 18.04 LTS and a machine running MacOS Catalina. It worked on both systems. . ExFat on Windows (Update) . After I wrote this blog post I was able to test my USB flash drive, formatted with exFat as described above, also on a machine running Windows 10. Unfortunately, I didn&#39;t work to my surprise. I&#39;m not really sure why it doesn&#39;t work, but on askubuntu.com I found another solution to format a USB flash drive as exFat, which does also work under Windows 10. In the following I want to describe the whole process again (on my Ubuntu 18.04 LTS) using this new solution. . If you haven&#39;t done so, you should install the exFat drivers on your Ubuntu. . sudo apt-get install exfat-fuse exfat-utils . Then, we need to plug our USB flash drive into our machine. It should be automatically recognized and mounted by Ubuntu. Again, an icon representing our USB flash drive should appear on the desktop. Alternatively, we should also find it under /media using the terminal (usually under /mnt on other Linux distributions). . Next, we need to find out the name of our drive. We can use fdisk for this as before. . sudo fdisk -l . As described above, the command should print a list of devices that are attached to our system. Pay very close attention to find the right drive name, since we are going to erase all data of that drive later. In the following let&#39;s assume again that our USB flash drive shows up as /dev/sdb in the device list and /dev/sdb1 indicates its first (and usually only) partition. When I ran the command, it showed up as follows. . Disk /dev/sdb: 14,4 GiB, 15476981760 bytes, 30228480 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0x36345114 Device     Boot Start      End  Sectors  Size Id Type /dev/sdb1        2048 30228479 30226432 14,4G  7 HPFS/NTFS/exFAT . Again, you can check if the total memory that is displayed for /dev/sdb matches your USB flash drive&#39;s memory. As described above it will most likely be a little less as advertised on your USB flash drive. If you want to read again why that is, please see above. . Besides fdisk there are also other tools to find out the device name. One of them is lsblk for instance. . lsblk . It should also print a device list and our USB flash drive should appear as /dev/sdb in the following way. . sdb                       8:16   1  14,4G  0 disk   └─sdb1                    8:17   1  14,4G  0 part . . Warning: Pay very close attention to find the right drive name, since we are going to erase all data stored on the drive! For instance, if you find a drive named /dev/sda (or something similar) in the device list, you must not choose this one! The drive /dev/sda is usually the drive that is used to boot GNU/Linux from. It is not your USB flash drive. You don&#8217;t want to format it! The correct drive could be /dev/sdb as in my example here but it doesn&#8217;t have to be the correct one on your system. . Next, we need to unmount our drive. . sudo umount /dev/sdb* . We should either get no output or the following output. No error should appear. . umount: /dev/sdb: not mounted. umount: /dev/sdb1: not mounted. . The icon on the desktop representing our USB flash drive should disappear and it should also not be mounted under /media (or /mnt on other Linux distributions) anymore. Additionally, we can also check with df whether /dev/sdb is still mounted. . df . df prints a list of mounted devices which shouldn&#39;t contain /dev/sdb* anymore. . . Warning: Make sure that you don&#8217;t have any data on the USB flash drive anymore that you still need. If there is still data on the drive that you still need, you have to make a backup of the data before you proceed. . Now we can delete the filesystem of our USB flash drive. To do this we can use wipefs for this again. . sudo wipefs -a /dev/sdb . Next, we need to create the new partition table. However, the partition table needs to be of the GPT type. fdisk that we used before to create the partition table seems to support it. However, I have only been able to create the correct partition table using a tool called parted (read more about it here). Maybe I will find out how to do the same thing using fdisk in the future but for now let&#39;s use parted. So, let&#39;s create the partition table and specify that it should be of type GPT. . sudo parted /dev/sdb mklabel gpt . After running the above command it could happen that you get the following message. . Information: You may need to update /etc/fstab. . In this thread on reddit.com someone says that this message is displayed by several partition editors when creating a new partition to indicate that you need to add it to /etc/fstab if you want to have it mounted on boot. Since we don&#39;t want to have it mounted when booting our machine (it is portable device and not our internal hard drive!), we can ignore this message I think. . . Note: GPT and MBR are two different types of partition tables. If you are interested in the details of both of these types, you can read more about them here and here. . Next, let&#39;s create our partition that encompasses the whole space of our USB flash drive. . sudo parted -a optimal /dev/sdb mkpart primary &#39;0%&#39; &#39;100%&#39; . The message regarding /etc/fstab could appear again but as mentioned above I think we can ignore it. Finally, we need to add a Microsoft related flag called msftdata (read more about it here) on our newly created partition in the following way. . sudo parted &lt;DEVICE&gt; set &lt;PARTITION_NUMBER&gt; msftdata on . In my case the command was as follows. . sudo parted /dev/sdb set 1 msftdata on . Again, the message regarding /etc/fstab can appear but as before you can probably ignore it. Now we can finally create the file system. . sudo mkfs.exfat -n bam098 /dev/sdb1 . Again, bam098 is the name that I decided to give my USB flash drive. You can use any name you like here. After running the command we should see the following or a similar output. . mkexfatfs 1.2.8 Creating... done. Flushing... done. File system created successfully. . Let&#39;s also make sure again that there are no errors. . sudo fsck.exfat /dev/sdb1 . We should see the following or a similar output. . exfatfsck 1.2.8 Checking file system on /dev/sdb1. File system version 1.0 Sector size 512 bytes Cluster size 32 KB Volume size 14 GB Used space 2 MB Available space 14 GB Totally 0 directories and 0 files. File system checking finished. No errors found. . We should make sure that it says No errors found. Additionally, we can also unplug the USB flash drive from our machine and plug it in again. When running df it should now appear in the list of mounted devices. . /dev/sdb1                    15112192      2048  15110144   1% /media/bam/bam098 . Finally, I tested my USB flash drive, formatted as exFat, on Ubuntu 18.04 LTS, MacOS Catalina and Windows 10. It worked on all three operating systems this time. So, we are done! .",
            "url": "https://bam098.github.io/ai-blog/usb%20flash%20drive/exfat/ubuntu/2020/04/20/format-exfat.html",
            "relUrl": "/usb%20flash%20drive/exfat/ubuntu/2020/04/20/format-exfat.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Access iPhone Photos on Ubuntu",
            "content": "I have an iPhone SE that I also use to take pictures. These pictures should also be accessible on my laptop running Ubuntu 18.04 LTS, since I want to store them on a portable hard drive disk or I want to use them on my laptop for specific experiments. Unfortunately, accessing photos from an iPhone on Ubuntu does not work out of the box. However, I found this great tutorial written by Ben Stockton that describes how you still can access iPhone photos on Ubuntu. In the following I will explain each step and the experience I made. If you rather want to follow the original tutorial, you can find it here. . First of all, we need to install the libimobiledevice library. This library makes it possible that our iPhone gets recognized by Ubuntu. . sudo apt install libimobiledevice6 libimobiledevice-utils . Now you can connect the iPhone with your laptop using a USB cable. For some reason it was also important to unlock my phone after I plugged in the USB cable. However, then only something called iPhone Documents got mounted. I&#39;m not sure what that is, but it doesn&#39;t give me access to the photos. Thus, I needed to paired my iPhone with my laptop using the following command. . idevicepair pair . This actually gave me an error. . ERROR: Could not validate with device **** because a passcode is set. Please enter the passcode on the device and retry. . The error tells us to allow the connection between the iPhone and Ubuntu first. To do this, we need to take our iPhone, which should display now whether we trust the connection request. We need to confirm this on the iPhone and type in our password. Now, we can try to pair the phone with Ubuntu again. Therefor, we need to go back to the laptop and run idevicepair pair again. This time we should get a success response. . SUCCESS: Paired with device **** . Next, we need to allow multiple connections between the iPhone and Ubuntu. To do this we can use usbmuxd. . usbmuxd -f -v . Then, we need to install ifuse, which allows us to mount the file system of our iPhone on Ubuntu. . sudo apt-get install ifuse . Now, we can mount the iPhone. To do this we need to specify where it should be mounted on our laptop. I created a new directory that I simply called iphone for this. . sudo mkdir /media/iphone . Then, we can mount the iPhone with ifuse. . sudo ifuse /media/iphone . Now, we should be able to access the files on the iPhone. We can take a look with the ls command. . sudo ls /media/iphone . It should list several folders. One of these folders should be called DCIM. This is the folder where our photos are stored. So, let&#39;s take a look into that folder using ls. . sudo ls /media/iphone/DCIM . We should see a list of several folders named ***APPLE (e.g. 100APPLE). Each of these folders contains some subset of our photos. . Additionally, if you also want to view your photos through a GUI-based program, you can use shotwell for instance. It should be pre-installed on Ubuntu. If it is not, then you can install it using the following command. . sudo apt-get install shotwell . Then, we can start shotwell over the terminal using sudo. . sudo shotwell . It was necessary to start shotwell this way, because starting it without sudo rights it didn&#39;t work for me to access the iPhone. shotwell should display our mounted iphone folder. When we click on it, it should automatically import the photos. If this doesn&#39;t happen, when can go to File and then Import from Folder in shotwell to important them manually. . When we are finished accessing our photos, we need to unmount the iPhone again before we disconnect it from the laptop. Ben Stockton describes in his tutorial that we can unmount using the command ifuse -u /media/iphone. However, this didn&#39;t work for me. I simply used the following. . sudo umount /media/iphone . Additionally, we might also unmount the iPhone Documents drive manually over Nautilus. Then, we can unplug our iPhone from our laptop and we are done! .",
            "url": "https://bam098.github.io/ai-blog/iphone/ubuntu/2020/04/19/access-iphone.html",
            "relUrl": "/iphone/ubuntu/2020/04/19/access-iphone.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Submitting to Kaggle",
            "content": "In my last blog post I showed how to train a CNN based model for classifying images of distracted drivers using the fastai library. The images came from the training set of the State Farm Distracted Driver Detection competition on Kaggle. In this blog post I want to show how to create predictions for the images of the competition&#39;s test set using the trained model and how to submit these predictions to Kaggle. . Neural network based models usually need to be trained on a CUDA suitable GPU. Unfortunately, my local machine doesn&#39;t contain such a GPU. However, there are cloud providers that offer machines with such GPUs. Google Colab is one of these providers that even offers an option for free GPU access. As a result, I used Google Colab to train my model. You can read in my last blog post how I did this. . Now, I need to use my trained model to make predictions for the images of the test set. Theoretically I could do this on Google Colab as well. However, in practice I encountered a problem. The test set of the State Farm Distracted Driver Detection competition contains a huge amount of images in the test set folder. This is problematic, because Google Colab seems to have problems accessing folders with many files. Unfortunately, I haven&#39;t found a good solution to this problem, yet. The only options that came to my mind were: 1) Splitting the test set into multiple groups and store each group in an own folder or 2) download my trained model from Google Colab to my local machine to create the predictions there. The first option has the disadvantage that I need to do the splitting on my local machine and upload each folder to Google Colab then, since I can&#39;t even access the test set folder on Google Colab for the splitting. This seems to be a lot of work and it could take a long time to upload the folders. The second option has the disadvantage that it will take a long time to create the predictions, because I only can use the CPU for this and not a GPU. To avoid time consuming experiments to find out how many files in a folder can be handled by Google Colab, I decided for the second option. It&#39;s also not a good option, but let&#39;s try it. . . Important: In this tutorial I switch from Google Colab to my local machine. As a result, I need to download my trained model from Google Colab as well as the dataset of the Kaggle competition to my local machine before running any code. . In the following the tutorial describes: . How to load the test set using fastai | How to create the predictions for the test set images using fastai | How to submit the predicitons to Kaggle using the Kaggle API | . Warning: Using the CPU to create predictions for a huge amount of test set images can take several hours. . . Note: If you used your local machine or a remote machine with a CUDA suitable GPU instead of Google Colab to train your model, you can keep using it. There is no need to switch to another machine, since there shouldn&#8217;t be any problems to open the test set folder. You can still use the following instructions though. They will just work fine. They will even work more quickly for you, since you have access to a GPU. . Load the Test Set . I described how to load the training data using the fastai library in my last blog post. Now, I want to show how to load the test data. However, since loading the test data is strongly coupled to loading the training data when using fastai, I&#39;m going to briefly review how to load the training data as well. . First, we need to run some magics and check the versions of PyTorch and fastai as always. We should make sure that we use the same versions of PyTorch and fastai that we also used for training the model on Google Colab. I recommend installing the libraries using conda on your local machine. You can read more about how to do that here. . %reload_ext autoreload %autoreload 2 %matplotlib inline import torch import torchvision import fastai print(&#39;torch version: {}&#39;.format(torch.__version__)) print(&#39;torchvision version: {}&#39;.format(torchvision.__version__)) print(&#39;fastai version: {}&#39;.format(fastai.__version__)) . torch version: 1.4.0 torchvision version: 0.5.0 fastai version: 1.0.60 . We can also check whether we have GPU access. On my local machine I don&#39;t. If you do, creating the predictions will run a lot faster for you later. . torch.cuda.is_available() . False . Then, we need to load some libraries. . from fastai.vision import * from fastai.metrics import accuracy import pandas as pd import random . Creating the validation set or initializing the model parameters are based on some kind of randomness. However, there are options to create the same random numbers again and again. In Python/PyTorch we can achieve that through the following code. . torch.manual_seed(0) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False random.seed(0) . You can read more about reproducibility in PyTorch here. . After downloading the dataset and the model that I trained on Google Colab, I made a project folder. In this project folder I created a data and an nbs subfolder. In the nbs subfolder I put the jupyter notebook that I used to run the code shown in this tutorial here. In the data subfolder I put the downloaded dataset. Furthermore, I needed to create a folder named models in the training folder of the downloaded dataset and put my model in there. This is necessary, since fastai looks for models in that folder. When we saved our model on Google Colab, fastai created that folder and stored the model file in there automatically. . Let&#39;s specify the path to the training and the path to the test set folder. . base_dir = &#39;../data/state-farm-ddd/&#39; train_ds_path = Path(base_dir + &#39;imgs/train&#39;) test_ds_path = Path(base_dir + &#39;imgs/test&#39;) . Let&#39;s take a look into the training folder. . train_ds_path.ls() . [PosixPath(&#39;../data/state-farm-ddd/imgs/train/c7&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c0&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c9&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c8&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c1&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c6&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/models&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c3&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c4&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c5&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/train/c2&#39;)] . As we can see, the training folder contains the 10 subfolders c0 to c9 as well as the models subfolder. In the subfolders c0 to c9 we can find the training images of the 10 classes of driver distraction. The models subfolder contains our model that we trained on Google Colab. . Let&#39;s also take a look into the test folder. . test_ds_path.ls() . [PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_60161.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_94786.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_85853.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_36327.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_39014.jpg&#39;), ...] . We have many images here. Notice, the test set is not split into multiple folders representing the image classes like the training set. The Kaggle competition asks us to predict these classes. . Then, we need to take a certain amount of images from the training set to create our validation set. To able to train a proper model we need to make sure that there isn&#39;t any driver that is shown on images of the training set and the validation set at the same time. See my last blog post for more details why this is important. To obtain the information which images contain which drivers (also called subjects) we need to load the driver_imgs_list.csv file into a pandas data frame. . df = pd.read_csv(base_dir + &#39;driver_imgs_list.csv&#39;); df . subject classname img . 0 p002 | c0 | img_44733.jpg | . 1 p002 | c0 | img_72999.jpg | . 2 p002 | c0 | img_25094.jpg | . 3 p002 | c0 | img_69092.jpg | . 4 p002 | c0 | img_92629.jpg | . ... ... | ... | ... | . 22419 p081 | c9 | img_56936.jpg | . 22420 p081 | c9 | img_46218.jpg | . 22421 p081 | c9 | img_25946.jpg | . 22422 p081 | c9 | img_67850.jpg | . 22423 p081 | c9 | img_9684.jpg | . 22424 rows × 3 columns . Let&#39;s see how many images our training set contains. . n = df.shape[0] print(&#39;total: {}&#39;.format(n)) . total: 22424 . Next, let&#39;s check how many different drivers our training set contains. . df_by_subject = df.groupby(&#39;subject&#39;) unique_subjects = list(df_by_subject.groups.keys()) num_subjects = len(unique_subjects) print(&#39;number of subjects: {}&#39;.format(num_subjects)) print(&#39;subjects: {}&#39;.format(unique_subjects)) . number of subjects: 26 subjects: [&#39;p002&#39;, &#39;p012&#39;, &#39;p014&#39;, &#39;p015&#39;, &#39;p016&#39;, &#39;p021&#39;, &#39;p022&#39;, &#39;p024&#39;, &#39;p026&#39;, &#39;p035&#39;, &#39;p039&#39;, &#39;p041&#39;, &#39;p042&#39;, &#39;p045&#39;, &#39;p047&#39;, &#39;p049&#39;, &#39;p050&#39;, &#39;p051&#39;, &#39;p052&#39;, &#39;p056&#39;, &#39;p061&#39;, &#39;p064&#39;, &#39;p066&#39;, &#39;p072&#39;, &#39;p075&#39;, &#39;p081&#39;] . Let&#39;s choose 20% of the drivers for our validation set. . valid_frac = 0.2 random.shuffle(unique_subjects) valid_subjects = unique_subjects[:int(num_subjects * valid_frac)] print(&#39;valid subjects: {}&#39;.format(valid_subjects)) . valid subjects: [&#39;p047&#39;, &#39;p002&#39;, &#39;p072&#39;, &#39;p052&#39;, &#39;p022&#39;] . However, we also should make sure that: . The sum of the images showing these selected drivers is approximately 20% of all training images | The set of images showing the selected drivers contains images of all classes in a similar amount | . df_valid = df.loc[df[&#39;subject&#39;].isin(valid_subjects)] n_valid = df_valid.shape[0] print(&#39;valid total: {} ({}%)&#39;.format(n_valid, round(100. * n_valid / n, 2))) . valid total: 3879 (17.3%) . It&#39;s not exactly 20%, but 17.3% should be okay. . df_valid.groupby([&#39;classname&#39;]).size().reset_index(name=&#39;counts&#39;) . classname counts . 0 c0 | 420 | . 1 c1 | 427 | . 2 c2 | 415 | . 3 c3 | 400 | . 4 c4 | 402 | . 5 c5 | 371 | . 6 c6 | 407 | . 7 c7 | 325 | . 8 c8 | 316 | . 9 c9 | 396 | . Furthermore, our set of selected images indeed contains images of each class in a similar amount. As a result, we can use this set as validation set. . Then we need to adjust the dataframe in the following way to be able to use it for loading the training data: . The img column doesn&#39;t only need to contain the image file names but also the file paths | We need an additional column is_valid that specifies whether an image belongs to the validation set or not (if not, it belongs to the training set) | . df[&#39;img&#39;] = df[&#39;classname&#39;] + &#39;/&#39; + df[&#39;img&#39;] # add is_valid column to indicate which images belong to the valid set df[&#39;is_valid&#39;] = df[&#39;subject&#39;].isin(valid_subjects) # remove subjects column df = df.drop(columns=[&#39;subject&#39;]); df . classname img is_valid . 0 c0 | c0/img_44733.jpg | True | . 1 c0 | c0/img_72999.jpg | True | . 2 c0 | c0/img_25094.jpg | True | . 3 c0 | c0/img_69092.jpg | True | . 4 c0 | c0/img_92629.jpg | True | . ... ... | ... | ... | . 22419 c9 | c9/img_56936.jpg | False | . 22420 c9 | c9/img_46218.jpg | False | . 22421 c9 | c9/img_25946.jpg | False | . 22422 c9 | c9/img_67850.jpg | False | . 22423 c9 | c9/img_9684.jpg | False | . 22424 rows × 3 columns . Next, we need to load the test data. We can load it as ImageList using the from_folder method. . test_ds = ImageList.from_folder(test_ds_path); test_ds . ImageList (79726 items) Image (3, 480, 640),Image (3, 480, 640),Image (3, 480, 640),Image (3, 480, 640),Image (3, 480, 640) Path: ../data/state-farm-ddd/imgs/test . Finally, we need to create a data bunch object using the data block API from fastai. The data bunch object contains the training, validation and test set as well as various data transformations. Actually, I created the data bunch object in my last blog post in almost the same way. However, now I also need to add add_test here to specify our test set. . bs = 32 tfms = get_transforms(do_flip=False) data = (ImageList .from_df(df, train_ds_path, cols=1) .split_from_df(col=2) .label_from_df(cols=0) .add_test(test_ds) .transform(tfms=tfms, size=299) .databunch(bs=bs) ).normalize(imagenet_stats) . Let&#39;s check which classes we have. . data.classes . [&#39;c0&#39;, &#39;c1&#39;, &#39;c2&#39;, &#39;c3&#39;, &#39;c4&#39;, &#39;c5&#39;, &#39;c6&#39;, &#39;c7&#39;, &#39;c8&#39;, &#39;c9&#39;] . These are the 10 classes of driver distraction as expected. Let&#39;s also take a look at a few images of our loaded data. . data.show_batch(rows=3, figsize=(9,8)) . Finally, let&#39;s check how many images our training, validation and test set contain. . len(data.train_ds) . 18545 . len(data.valid_ds) . 3879 . len(data.test_ds) . 79726 . Create Predictions . After loading the data we have to create the learner. We need to make sure that we use the same model architecture here that we already used for training our model on Google Colab. Otherwise we won&#39;t be able to load our model that we trained on Google Colab. I used a ResNet50. . learn = cnn_learner(data, models.resnet50, metrics=accuracy) . Now, we can load our model. . learn.load(&#39;stage-2&#39;); . To make sure that we loaded the model successfully we can check its performance on the validation set using the validate method. Since I only could use the CPU on my local machine to check model performance on the validation set, it took about 30 minutes to run through. . learn.validate() . [0.33628428, tensor(0.9330)] . As result we get two numbers. The first number is the loss reached on the validation set. The second number is the accuracy of our model on the validation set. Since an accuracy of more than 93% was also the accuracy of our final model last time, we can presume that the model was loaded successfully. . Now, let&#39;s use the model to make predictions for the test set images. Therefor we need to use the get_preds method. It took approximately 12 hours to run through. . probs, _ = learn.get_preds(ds_type=DatasetType.Test) probs.shape . torch.Size([79726, 10]) . As result we obtain the 10 class probabilities and the target (not prediction!) for each image. However, since we don&#39;t have the targets for our test images, we only get a zero target for each image. Thus, I omit the targets using _. . To obtain the predictions we simply need to check for each image which of the 10 class reached the highest probability. . probs_npy = probs.detach().numpy() np.argmax(probs_npy, axis=1) . array([5, 6, 1, 1, ..., 4, 8, 1, 0]) . To be on the safe side I also decided to store the predictions in a file. Since it took 12 hours to compute them, I wouldn&#39;t want to compute them again! . np.save(&#39;statefarm_probs.npy&#39;, probs_npy) . Submit to Kaggle . Now we are ready to submit our predictions to Kaggle. Let&#39;s check in which format the competition expects our predictions. Therefor we can check the competition&#39;s website or the sample submission file that is part of the downloaded dataset. . pd.read_csv(&#39;../data/state-farm-ddd/sample_submission.csv&#39;) . img c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 . 0 img_1.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 1 img_10.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 2 img_100.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 3 img_1000.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 4 img_100000.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 79721 img_99994.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 79722 img_99995.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 79723 img_99996.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 79724 img_99998.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 79725 img_99999.jpg | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | . 79726 rows × 11 columns . As we can see, we need to submit a CSV file to Kaggle containing the 10 probabilities for each test image as well as the test image file name. First, if necessary, let&#39;s load our prediction probabilities again. . probabilities = np.load(&#39;statefarm_probs.npy&#39;) probabilities.shape . (79726, 10) . Next, we need to get the file names of the test images. The data bunch object contains the paths of the test images. . data.test_ds.items . array([PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_60161.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_94786.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_85853.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_36327.jpg&#39;), ..., PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_68524.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_67617.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_30997.jpg&#39;), PosixPath(&#39;../data/state-farm-ddd/imgs/test/img_21642.jpg&#39;)], dtype=object) . However, we only need the file names. Let&#39;s extract them from the paths. . get_file_name = lambda p: p.name vfunc = np.vectorize(get_file_name) img_names = pd.Series(vfunc(data.test_ds.items)); img_names . 0 img_60161.jpg 1 img_94786.jpg 2 img_85853.jpg 3 img_36327.jpg 4 img_39014.jpg ... 79721 img_77404.jpg 79722 img_68524.jpg 79723 img_67617.jpg 79724 img_30997.jpg 79725 img_21642.jpg Length: 79726, dtype: object . Next, let&#39;s put the probabilities into a pandas data frame. . df = pd.DataFrame(probabilities) df.columns = data.classes . Then, let&#39;s add an additional column containing the image file names. . df[&#39;img&#39;] = img_names; df . c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 img . 0 9.064603e-08 | 9.556512e-10 | 1.208677e-08 | 6.626375e-06 | 4.161629e-07 | 9.980783e-01 | 1.425728e-07 | 2.311186e-06 | 1.048160e-03 | 8.639126e-04 | img_60161.jpg | . 1 1.341862e-11 | 1.325500e-11 | 2.737421e-09 | 1.233930e-09 | 2.569607e-10 | 2.989499e-08 | 1.000000e+00 | 2.460269e-11 | 8.222596e-11 | 1.081464e-11 | img_94786.jpg | . 2 1.278042e-07 | 9.999648e-01 | 1.621474e-07 | 1.249684e-06 | 2.125959e-06 | 1.716223e-08 | 3.776129e-09 | 3.145694e-05 | 1.170369e-07 | 9.460091e-09 | img_85853.jpg | . 3 1.983368e-07 | 9.999930e-01 | 6.280829e-06 | 1.208340e-08 | 4.862862e-08 | 5.451320e-09 | 7.702554e-08 | 2.440847e-09 | 2.714422e-07 | 1.117997e-07 | img_36327.jpg | . 4 5.995354e-04 | 2.111001e-07 | 1.837754e-07 | 1.218117e-03 | 3.950001e-07 | 9.981633e-01 | 1.178386e-06 | 1.751037e-07 | 1.668345e-05 | 3.223242e-07 | img_39014.jpg | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 79721 4.511207e-01 | 5.139207e-02 | 3.446462e-03 | 2.381562e-03 | 2.746080e-04 | 2.520865e-01 | 2.002078e-01 | 4.145120e-03 | 1.319973e-02 | 2.174546e-02 | img_77404.jpg | . 79722 7.303311e-06 | 1.809456e-05 | 2.658817e-06 | 1.319719e-03 | 9.974797e-01 | 3.963906e-07 | 1.893191e-05 | 8.105447e-05 | 9.921284e-04 | 7.999525e-05 | img_68524.jpg | . 79723 3.712073e-07 | 5.125133e-07 | 1.103094e-05 | 1.346865e-06 | 9.106328e-07 | 4.559149e-07 | 9.792532e-02 | 8.688941e-06 | 9.020439e-01 | 7.445801e-06 | img_67617.jpg | . 79724 1.119303e-02 | 9.558834e-01 | 7.588323e-04 | 2.264025e-02 | 1.162238e-03 | 9.801583e-04 | 7.104538e-03 | 3.813615e-05 | 1.180822e-05 | 2.274428e-04 | img_30997.jpg | . 79725 9.999969e-01 | 2.245683e-09 | 2.918841e-08 | 1.102728e-07 | 3.436966e-08 | 1.067264e-08 | 8.381981e-11 | 1.447656e-09 | 9.670017e-08 | 2.855863e-06 | img_21642.jpg | . 79726 rows × 11 columns . However, the column containing the image file names must be the first column. Let&#39;s change the column order. . cols = df.columns.tolist() cols = [cols[-1]] + cols[:-1]; cols . [&#39;img&#39;, &#39;c0&#39;, &#39;c1&#39;, &#39;c2&#39;, &#39;c3&#39;, &#39;c4&#39;, &#39;c5&#39;, &#39;c6&#39;, &#39;c7&#39;, &#39;c8&#39;, &#39;c9&#39;] . df = df.reindex(columns=cols); df . img c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 . 0 img_60161.jpg | 9.064603e-08 | 9.556512e-10 | 1.208677e-08 | 6.626375e-06 | 4.161629e-07 | 9.980783e-01 | 1.425728e-07 | 2.311186e-06 | 1.048160e-03 | 8.639126e-04 | . 1 img_94786.jpg | 1.341862e-11 | 1.325500e-11 | 2.737421e-09 | 1.233930e-09 | 2.569607e-10 | 2.989499e-08 | 1.000000e+00 | 2.460269e-11 | 8.222596e-11 | 1.081464e-11 | . 2 img_85853.jpg | 1.278042e-07 | 9.999648e-01 | 1.621474e-07 | 1.249684e-06 | 2.125959e-06 | 1.716223e-08 | 3.776129e-09 | 3.145694e-05 | 1.170369e-07 | 9.460091e-09 | . 3 img_36327.jpg | 1.983368e-07 | 9.999930e-01 | 6.280829e-06 | 1.208340e-08 | 4.862862e-08 | 5.451320e-09 | 7.702554e-08 | 2.440847e-09 | 2.714422e-07 | 1.117997e-07 | . 4 img_39014.jpg | 5.995354e-04 | 2.111001e-07 | 1.837754e-07 | 1.218117e-03 | 3.950001e-07 | 9.981633e-01 | 1.178386e-06 | 1.751037e-07 | 1.668345e-05 | 3.223242e-07 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 79721 img_77404.jpg | 4.511207e-01 | 5.139207e-02 | 3.446462e-03 | 2.381562e-03 | 2.746080e-04 | 2.520865e-01 | 2.002078e-01 | 4.145120e-03 | 1.319973e-02 | 2.174546e-02 | . 79722 img_68524.jpg | 7.303311e-06 | 1.809456e-05 | 2.658817e-06 | 1.319719e-03 | 9.974797e-01 | 3.963906e-07 | 1.893191e-05 | 8.105447e-05 | 9.921284e-04 | 7.999525e-05 | . 79723 img_67617.jpg | 3.712073e-07 | 5.125133e-07 | 1.103094e-05 | 1.346865e-06 | 9.106328e-07 | 4.559149e-07 | 9.792532e-02 | 8.688941e-06 | 9.020439e-01 | 7.445801e-06 | . 79724 img_30997.jpg | 1.119303e-02 | 9.558834e-01 | 7.588323e-04 | 2.264025e-02 | 1.162238e-03 | 9.801583e-04 | 7.104538e-03 | 3.813615e-05 | 1.180822e-05 | 2.274428e-04 | . 79725 img_21642.jpg | 9.999969e-01 | 2.245683e-09 | 2.918841e-08 | 1.102728e-07 | 3.436966e-08 | 1.067264e-08 | 8.381981e-11 | 1.447656e-09 | 9.670017e-08 | 2.855863e-06 | . 79726 rows × 11 columns . Okay. Finally, our data frame looks as expected. Let&#39;s save it as a CSV file. . df.to_csv(&#39;submission.csv&#39;, index=False) . Let&#39;s load it again to make sure everything still looks fine. . submission = pd.read_csv(&#39;submission.csv&#39;); submission . img c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 . 0 img_60161.jpg | 9.064603e-08 | 9.556512e-10 | 1.208677e-08 | 6.626375e-06 | 4.161629e-07 | 9.980784e-01 | 1.425728e-07 | 2.311186e-06 | 1.048160e-03 | 8.639126e-04 | . 1 img_94786.jpg | 1.341862e-11 | 1.325500e-11 | 2.737421e-09 | 1.233930e-09 | 2.569607e-10 | 2.989499e-08 | 1.000000e+00 | 2.460269e-11 | 8.222596e-11 | 1.081464e-11 | . 2 img_85853.jpg | 1.278042e-07 | 9.999648e-01 | 1.621474e-07 | 1.249684e-06 | 2.125959e-06 | 1.716222e-08 | 3.776129e-09 | 3.145694e-05 | 1.170369e-07 | 9.460091e-09 | . 3 img_36327.jpg | 1.983368e-07 | 9.999930e-01 | 6.280829e-06 | 1.208340e-08 | 4.862863e-08 | 5.451320e-09 | 7.702554e-08 | 2.440847e-09 | 2.714422e-07 | 1.117997e-07 | . 4 img_39014.jpg | 5.995354e-04 | 2.111001e-07 | 1.837754e-07 | 1.218117e-03 | 3.950001e-07 | 9.981633e-01 | 1.178386e-06 | 1.751037e-07 | 1.668344e-05 | 3.223242e-07 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 79721 img_77404.jpg | 4.511207e-01 | 5.139207e-02 | 3.446462e-03 | 2.381562e-03 | 2.746080e-04 | 2.520865e-01 | 2.002078e-01 | 4.145120e-03 | 1.319973e-02 | 2.174546e-02 | . 79722 img_68524.jpg | 7.303311e-06 | 1.809456e-05 | 2.658817e-06 | 1.319719e-03 | 9.974797e-01 | 3.963906e-07 | 1.893191e-05 | 8.105447e-05 | 9.921284e-04 | 7.999525e-05 | . 79723 img_67617.jpg | 3.712073e-07 | 5.125133e-07 | 1.103094e-05 | 1.346866e-06 | 9.106328e-07 | 4.559149e-07 | 9.792532e-02 | 8.688941e-06 | 9.020439e-01 | 7.445801e-06 | . 79724 img_30997.jpg | 1.119303e-02 | 9.558834e-01 | 7.588323e-04 | 2.264025e-02 | 1.162238e-03 | 9.801583e-04 | 7.104538e-03 | 3.813615e-05 | 1.180822e-05 | 2.274428e-04 | . 79725 img_21642.jpg | 9.999969e-01 | 2.245683e-09 | 2.918841e-08 | 1.102728e-07 | 3.436966e-08 | 1.067264e-08 | 8.381980e-11 | 1.447656e-09 | 9.670017e-08 | 2.855863e-06 | . 79726 rows × 11 columns . Now, we can submit the created CSV file containing our predictions to Kaggle. To do this we usually have two options. We could either manually upload it over the website or we could use the Kaggle API. For some reason I couldn&#39;t manually upload over the website. Thus, I used the Kaggle API. . ! kaggle competitions submit state-farm-distracted-driver-detection -f submission.csv -m &quot;first submission&quot; . 100%|███████████████████████████████████████| 11.0M/11.0M [00:47&lt;00:00, 242kB/s] Successfully submitted to State Farm Distracted Driver Detection . . I reached a score of 0.57751. This number doesn&#39;t express the accuracy but the multi-class logarithmic loss, which is the evaluation metric used for this competition. Since the competition is already closed, my submission doesn&#39;t appear on the competition&#39;s leaderboard. However, if we check the private leaderboard on the competition website, we can see that our model would be among the first 360 submissions. This is not a bad result for such a quick solution! There are a few options to improve our model. You can find a few ideas here, here and here. .",
            "url": "https://bam098.github.io/ai-blog/kaggle/image%20classification/fastai/cnn/google%20colab/2020/04/07/submitting-to-kaggle.html",
            "relUrl": "/kaggle/image%20classification/fastai/cnn/google%20colab/2020/04/07/submitting-to-kaggle.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Classifying Images of Distracted Drivers",
            "content": ". Many car accidents are caused by distracted drivers. Driving assistance systems could help to reduce that number. A camera could observe the driver and initiate an alert when the driver is distracted while driving. To recognize when the driver is distracted the driving system needs to examine every image frame coming from the video recording of the camera. This ends up being an image classification problem. For every incoming image frame we need to classify whether the image shows a distracted or a non-distracted driver. . A few years ago there was a competition on Kaggle that asked to solve a very similar problem. Although the competion is not active anymore, you can still download the dataset and submit results. The competition provides a dataset of images showing distracted drivers. However, here it was asked not to only classify distracted and non-distracted drivers but to also distinguish between the different kinds of distractions. The dataset encompasses the following classes: . c0: safe driving | c1: texting (right hand) | c2: talking on the phone (right hand) | c3: texting (left hand) | c4: talking on the phone (left hand) | c5: operating the radio | c6: drinking | c7: reaching behind | c8: hair and makup | c9: talking to passenger | . I already showed how to download the dataset on Google Colab in my last blog post. I put the download into the folder /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd. It contains the following files and directories: . img directory: folder containing the training and test images | sample_submission.csv: an example file showing how to submit to Kaggle | driver_imgs_list.csv: a CSV file containing the file name of each training image, its class id (c0, c1, ...) and the id of the person on the image | . Before we start using a notebook in Google Colab we need to select the GPU as the hardware accelerator. Otherwise training a model will take a really long time. To do this go in Google Colab to Runtime, then Change runtime type and finally select GPU as Hardware accelerator in the upcoming menue. Now, we have GPU access to train our image classification model. . . . Then, we need to mount our Google Drive space again to access the data. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) root_dir = &quot;/content/gdrive/My Drive/&quot; base_dir = root_dir + &#39;fastai-v3/data/state-farm-ddd/&#39; . Mounted at /content/gdrive . I want to use the fastai library for training our model, since it makes it possible to train an image classification model in a quick and easy way. Fastai is built on top of PyTorch. Both libraries are already pre-installed on Google Colab. However, to make sure that we have the lastest version of both libraries I want to re-install them. Fastai offers a script to setup its library on Google Colab. You can find more detailed setup instructions here. . ! pip uninstall torch torchvision fastai -y . Uninstalling torch-1.4.0: Successfully uninstalled torch-1.4.0 Uninstalling torchvision-0.5.0: Successfully uninstalled torchvision-0.5.0 Uninstalling fastai-1.0.60: Successfully uninstalled fastai-1.0.60 . ! curl -s https://course.fast.ai/setup/colab | bash . Updating fastai... Done. . Next, we need to run some some magics. We usually should do that at the beginning of every notebook. They are responsible for showing matplotlib plots inside the notebook and for automatic reloading the notebook if some underlying library code changes. . %reload_ext autoreload %autoreload 2 %matplotlib inline . Let&#39;s also check the versions of PyTorch and fastai. . import torch import torchvision import fastai . print(&#39;torch version: {}&#39;.format(torch.__version__)) print(&#39;torchvision version: {}&#39;.format(torchvision.__version__)) print(&#39;fastai version: {}&#39;.format(fastai.__version__)) . torch version: 1.4.0 torchvision version: 0.5.0 fastai version: 1.0.60 . Furthermore, we should also check if we really have access to the GPU. Google Colab does offer GPU access and we already enabled it. However, to make sure there aren&#39;t any problems here we can check whether a GPU is available by running the following command. . torch.cuda.is_available() . True . Then, we need to import the necessary fastai libraries as well as a few other libraries like e.g. Pandas that will be helpful later. . from fastai.vision import * from fastai.metrics import accuracy import pandas as pd import random . I&#39;m going to use a neural network based model, since this is the current-state-of-the-art for image classification. However, training such a model also involves some randomness. For instance, the parameters (also called weights) of the model are usually intialized involving random numbers. As a result, each time you train a model you obtain a different result although you use the same dataset and hyper-parameters for model training. Usually the differences won&#39;t be too big. However, if you still want to obtain the same result every time, you can run the following commands to achieve this (read more about it here). . torch.manual_seed(0) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False random.seed(0) . Now we are ready to load the dataset and train an image classification model using fastai. . Load Dataset . The imgs folder has two subfolders: trainand test. The test folder contains the test set, which needs to be classified by our model at the end. The resulting classifications should be sumitted to Kaggle then. However, before we can do that we need to train the model first. The training set required for model training is located in the train folder. . ds_path = Path(base_dir + &#39;imgs/train&#39;); ds_path . PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train&#39;) . Let&#39;s look into that folder. . ds_path.ls() . [PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c0&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c1&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c2&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c3&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c4&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c5&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c6&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c7&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c8&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/c9&#39;), PosixPath(&#39;/content/gdrive/My Drive/fastai-v3/data/state-farm-ddd/imgs/train/models&#39;)] . As we can see the train folder contains ten subfolders. Each subfolder contains images of a certain class. . Now, we need to load all the images from all of these ten folders and split them into a set used for training the model and a set used for validating the model. Usually we would do that by simply dividing the dataset randomly into two groups (e.g. 80% for the train set and 20% for the valid set). . However, it&#39;s actually not a good idea to do the splitting of the dataset randomly in this case here. We rather should divide it according the drivers that are on the images. We should not have a single person that is in the training and the validation set at the same time. Instead, we should divide the dataset into two sets of images showing different groups of people. The reason is that otherwise our model could overfit later. It could learn to focus on specific properties of the people (e.g. clothes, hair) instead of the state of distraction. However, the properties of the people are irrelevant for the classification problem here. The states of distraction are important. . Tip: You can read more about how to create validation sets in general in this really nice blog post written by Rachel Thomas from fast.ai. . But how can we do the separation depending on the drivers? As already mentioned above the CSV file driver_imgs_list.csv contains an id of a person for each image showing that person. This represents our drivers. So, let&#39;s look at this file using Pandas. The person id is represented by the subject column. . df = pd.read_csv(base_dir + &#39;driver_imgs_list.csv&#39;); df . subject classname img . 0 p002 | c0 | img_44733.jpg | . 1 p002 | c0 | img_72999.jpg | . 2 p002 | c0 | img_25094.jpg | . 3 p002 | c0 | img_69092.jpg | . 4 p002 | c0 | img_92629.jpg | . ... ... | ... | ... | . 22419 p081 | c9 | img_56936.jpg | . 22420 p081 | c9 | img_46218.jpg | . 22421 p081 | c9 | img_25946.jpg | . 22422 p081 | c9 | img_67850.jpg | . 22423 p081 | c9 | img_9684.jpg | . 22424 rows × 3 columns . We have three columns: . subject: the id of the person on the image | classname: the name of the class represeting the state of distraction | img: the file name of the image | . Let&#39;s see how many images we have in total. To get this number we need to count the number of rows of the table above. . n = df.shape[0] print(&#39;total: {}&#39;.format(n)) . total: 22424 . Next, let&#39;s check how many different drivers (i.e. subjects) we have. . df_by_subject = df.groupby(&#39;subject&#39;) unique_subjects = list(df_by_subject.groups.keys()) num_subjects = len(unique_subjects) print(&#39;number of subjects: {}&#39;.format(num_subjects)) print(&#39;subjects: {}&#39;.format(unique_subjects)) . number of subjects: 26 subjects: [&#39;p002&#39;, &#39;p012&#39;, &#39;p014&#39;, &#39;p015&#39;, &#39;p016&#39;, &#39;p021&#39;, &#39;p022&#39;, &#39;p024&#39;, &#39;p026&#39;, &#39;p035&#39;, &#39;p039&#39;, &#39;p041&#39;, &#39;p042&#39;, &#39;p045&#39;, &#39;p047&#39;, &#39;p049&#39;, &#39;p050&#39;, &#39;p051&#39;, &#39;p052&#39;, &#39;p056&#39;, &#39;p061&#39;, &#39;p064&#39;, &#39;p066&#39;, &#39;p072&#39;, &#39;p075&#39;, &#39;p081&#39;] . Let&#39;s choose a certain amount of drivers for the validation set and use the remaining drivers for the training set. Usual splits into training and validation sets in percent are 80/20, 70/30 or 60/40. For now, let&#39;s take 20% of the drivers for validation and 80% for training and see how good we can get with this split. The 20% for the validation set should be chosen randomly. Therefor we can use the random package from Python. . valid_frac = 0.2 random.shuffle(unique_subjects) valid_subjects = unique_subjects[:int(num_subjects * valid_frac)] print(&#39;valid subjects: {}&#39;.format(valid_subjects)) . valid subjects: [&#39;p047&#39;, &#39;p002&#39;, &#39;p072&#39;, &#39;p052&#39;, &#39;p022&#39;] . We chose 20% of the drivers which makes five. However, we also need to make sure that there are a sufficient number of images showing these five drivers compared to the images showing the remaining drivers. If our five chosen drivers only appear on e.g. 50 images, our train-test-split will be quite bad, since this is a tiny amount of images compared to the total amount of 22424. As a result, let&#39;s check on how many images our five drivers appear. . df_valid = df.loc[df[&#39;subject&#39;].isin(valid_subjects)] n_valid = df_valid.shape[0] print(&#39;valid total: {} ({}%)&#39;.format(n_valid, round(100. * n_valid / n, 2))) . valid total: 3879 (17.3%) . The fraction of the images is not quite 20% but 17% is okay for now. Next, let&#39;s also check whether there are enough images per class. It won&#39;t be good if our validation set doesn&#39;t contain a sufficient amount of images of each class. . df_valid.groupby([&#39;classname&#39;]).size().reset_index(name=&#39;counts&#39;) . classname counts . 0 c0 | 420 | . 1 c1 | 427 | . 2 c2 | 415 | . 3 c3 | 400 | . 4 c4 | 402 | . 5 c5 | 371 | . 6 c6 | 407 | . 7 c7 | 325 | . 8 c8 | 316 | . 9 c9 | 396 | . Okay, the amount of images per class don&#39;t differ too much between the different classes. So, our validation set seems to be okay. As a result, we can finally load the training and the validation set now. . As I already mentioned above I want to use the fastai library to load the data. The fastai library offers several ways to do that. You can read more about it in its documentation. However, since we already have a Pandas dataframe, it seems to me that loading the data using that dataframe is the easiest option. Moreover, through a dataframe we can also handle the creation of the validation set according to the drivers in an easy way. . Before we can load the data we need to adjust our dataframe a bit. The values of the img column need to include the path to each image and not only the image file name. Then, to create a validation set according to the drivers later we need to add a new column with boolean values to our dataframe indicating whether an image belongs to the validation set or not (if not, it belongs to the training set). I want to call this new column is_valid. After creating this new column we can remove the subject column from the dataframe. . . Warning: A new version of the fastai library is going to be released in July 2020. It sounds like that there will be quite some changes in the new version. As a result, the code in this tutorial using fastai v1 will require to be updated in the future to also work under fastai v2. . df[&#39;img&#39;] = df[&#39;classname&#39;] + &#39;/&#39; + df[&#39;img&#39;] # add is_valid column to indicate which images belong to the valid set df[&#39;is_valid&#39;] = df[&#39;subject&#39;].isin(valid_subjects) # remove subjects column df = df.drop(columns=[&#39;subject&#39;]) . df . classname img is_valid . 0 c0 | c0/img_44733.jpg | True | . 1 c0 | c0/img_72999.jpg | True | . 2 c0 | c0/img_25094.jpg | True | . 3 c0 | c0/img_69092.jpg | True | . 4 c0 | c0/img_92629.jpg | True | . ... ... | ... | ... | . 22419 c9 | c9/img_56936.jpg | False | . 22420 c9 | c9/img_46218.jpg | False | . 22421 c9 | c9/img_25946.jpg | False | . 22422 c9 | c9/img_67850.jpg | False | . 22423 c9 | c9/img_9684.jpg | False | . 22424 rows × 3 columns . Now we can load the dataset using the data blocks API of fastai. This encompasses several building blocks: . Create an image list from the img column (index: 1) of our dataframe | Split the dataset into training and validation set using the is_valid column (index: 2) of the dataframe | Load the labels for each image from the classname column (index: 0) of the dataframe | Apply any transformations to the images like e.g. data augmentation or resizing of the images | Specify a certain batch size and create a databunch object | Normalize the images | . Due to technical reasons, it is necessary that all of our images are of the same size for model training. Moreover, they also shouldn&#39;t be too large. As a result, we need to resize all of them. Since I want to use transfer learning later to train our model, it is usually a good idea to set the image size to 224x224x3. This is caused by the fact that most pre-trained models that are used for transfer learning are trained with images of this size. However, since the images of our dataset contain a lot of details that are import for classifying them, I want to go a bit bigger here. As a result, I decided to use a size of 299x299x3. I probably don&#39;t want to go bigger than that for now, because bigger images also increase model training time and memory consumption later. . When training the model, the images are fed into the network in batches. The batch size is actually a hyper-parameter that will influence model training. Usually smaller batch sizes have a regularization effect on the model. However, I&#39;m going to use the 1-cycle-policy to train our model, which should be used together with a batch size as large as possible (read this paper for more details). As a result I set the batch size to 32. You can also try to increase the batch size, but at some point you will run out of GPU memory. . However, besides feeding the original images into the neural network it is also a good idea to change these images a little bit and use the resulting new images for training as well. This way we are able to get more variation into our training set which is an additonal way to avoid model overfitting. There are many ways to change the images. All of these techniques are summarized under the term data augmentation. I don&#39;t want to go into too much detail now, but you can read more about it here. Fastai offers some standard data augmentation operations which we can load with the method get_transforms. The only operation which we might not want to use is flipping the images. Otherwise our model might get confused with e.g. images of the classes texting (right hand) and texting (left hand). . After creating these additional images through data augmentation, we need to normalize our whole dataset (i.e. our original images and the images created through data augmentation) in a last step. Normalization often improves model training. You can find more information about how to normalize data and why it is useful here and here. For now it is only important to know that we need the following two statistics to normalize our images: a mean to shift and a standard deviation to scale the image data. We can either calculate mean and standard deviation from our own data or we can use a pre-computed mean and standard deviation from a standard dataset like ImageNet. Since ImageNet has a lot more data, its mean and standard deviation are probably more accurate. Furthermore, our pre-trained model that we are going to use was also pre-trained on ImageNet. Thus it probably also makes sense to use its image statics. . Okay, let&#39;s see how all this looks in code using the fastai data block API. . bs = 32 tfms = get_transforms(do_flip=False) data = (ImageList .from_df(df, ds_path, cols=1) .split_from_df(col=2) .label_from_df(cols=0) .transform(tfms=tfms, size=299) .databunch(bs=bs) ).normalize(imagenet_stats) . . Tip: You can find information about the data block API of fastai v1 in the current documentation. Furthermore, there is also already some information about the data block API of fastai v2 in this blog post of Zach Mueller and this paper from Jeremy Howard and Sylvain Gugger. . Now we can check which different classes our loaded data has. These should be the ten classes mentioned above. . data.classes . [&#39;c0&#39;, &#39;c1&#39;, &#39;c2&#39;, &#39;c3&#39;, &#39;c4&#39;, &#39;c5&#39;, &#39;c6&#39;, &#39;c7&#39;, &#39;c8&#39;, &#39;c9&#39;] . We can also look at some sample images of the dataset. . data.show_batch(rows=3, figsize=(9,8)) . To make sure we loaded all images of the dataset we can print the size of the training and validation set. . len(data.train_ds) . 18545 . len(data.valid_ds) . 3879 . As we can see our training set contains 18545 images and our validation set contains 3879 images. In total we have 22424 images. This means we loaded all the images. Moreover, our splitting of the dataset is correct as well. Now, we can train a model using this data. . Train Model . Before we can train a neural network based model we need to decide which network architecture we want to use. For image classification we usually use a Convolutional Neural Network (often abbreviated with CNN). The most common CNN based architecture for image classification nowadays is a Residual Network based architecture (often abbreviated with ResNet). Fastai offers such an architecture in several sizes. I decided to use a ResNet50 (i.e. it consists of 50 layers) here, since it is a standard model architecture for image classification. . Furthermore, as already mentioned above I don&#39;t want to train a model from scratch, since this could be very complicated. I rather want to use a pre-trained model. This process is called transfer learning. The advantage of using transfer learning is that we don&#39;t need to start training our model using zero knowledge. Although the pre-trained model was trained with a totally different dataset (usually ImageNet) it probably contains at least some useful information. To see this we need to understand how neural network based training works. Again, I don&#39;t want to go into too much detail here, but you can think of it as follows. To train a neural network the network needs to look at many different images and try to identify class specific features in these images in a hierachical way. It starts by searching for low level features like e.g. edges and then combines these to simple shaps. These simple shaps are further combined to more complex shapes and so on. At the end the network hopefully finds highly characteristic features for each class (you can read more about it here). Although these characteristic features most likely won&#39;t be useful for our dataset, the lower level features (e.g. edges) probably will be. Thus, transfer learning is always recommended to use. As a result, in fastai when selecting a standard model architecture it will use the corresponding pre-trained model as default. . But how can we train a model with fastai? To be able to train a model we need to create a model learner first. It requires our data, the model architecture (with the pre-trained model weights) and a metric to evaluate the model as input. I decided to use accuracy as the metric here, since it is a standard metric that is often used in practice. . learn = cnn_learner(data, models.resnet50, metrics=accuracy) . To display the model architecture we simply can run the following code. . learn.model . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ... . When training a model we need to set several hyper-parameters. I already mentioned the batch size. However, there are a few more hyper-parameters that we can set. Fastai uses some default values for most of them which I don&#39;t want to change for now. The only hyper-parameter we do need to set besides the batch size is the most important one: the learning rate. The learning rate is responsible for how fast we can train our model. The higher our learning rate the faster we usually can train the model. But be careful! The value for the learning rate also mustn&#39;t be too high, because learning rates that are too high could corrupt model training. . There are several ways to set the learning rate. We will use a technique called 1-cycle-policy here that usually gives quite promising results. Again, I don&#39;t want to go into too much detail here how it exactly works. Fastai handles it for us, so that we usually don&#39;t need to care about it. As a result, in the following I only want to give a brief overview here. . As I already mentioned, during model training we show the network every image of the dataset. The model can use the information from the images to improve a little bit. Then we show it all the images again and it can improve a little bit further. The time between showing all the images to the model and using the information of the images to improve the model is called an epoch. Usually we use several epochs to train a model. Sometimes we even need many epochs. This might be necessary because the classification problem is quite difficult. However, another reason could be that we didn&#39;t choose good values for the hyper-parameters. Especially the learning rate. As I already mentioned above a too small learning rate could result in very slow training. But how can we find a good learning rate? The 1-cycle-policy does it as follows. It starts with a certain value for the learning rate. Then, it linearly increases it to a maximum value after half of our total number of specified epochs. From there the learning rate is decreased again during the remaining epochs. There are a lot more details here. If you are interested, you can read more about it in this blog post. . As I already mentioned, since we use the fastai library, we don&#39;t need to care about how the 1-cycle-policy works. However, the only thing we do need to specify is the maximum learning rate for it. To find this maximum learning rate we can use a learning rate finder. A learning rate finder looks for the maximum learning rate that is still useful for training. It trains a model for a few batches while increasing the learning rate. In fastai we can use the learning rate finder in the following way. . learn.lr_find() learn.recorder.plot() . The learning rate finder gives us a plot showing the increasing learning rates and their corresponding losses. At some point the losses explode. But what are losses? . Losses show us how good our model currently is (a lower value is better here). So, isn&#39;t it similar to the accuracy then? Yes, it is. However, the loss is used by the model to see how good it is and to improve itself. For mathematical reasons, metrics like e.g. the accuracy cannot be used for this. Metrics are only used for humans, since they are more easily human-interpretable than losses. . Okay, let&#39;s get back to our plot! The minimum loss value is reached with a learning rate of 0.1. After that the loss explodes which means that a learning rate greater than 0.1 is too high. So, should we use 0.1 as our maximum learning rate? No! We rather should choose a value ten times smaller than the minimum as our maximum learning rate. This results in a value of 0.01. You can read more about why that is here. . Okay, let&#39;s train our model for a few epochs using the 1-cycle-policy with a maximal learning rate of 0.01. Usually it&#39;s good to train the model for at least four epochs. Let&#39;s use a few more epochs than four. . . Warning: I observed that sometimes training on Google Colab can be very slow. I&#8217;m not sure what the problem is. The only reason I can think of is that many people use Google Colab at the same time. However, independently of what the reason is, when training is very slow, you just need to wait a bit and try again. . learn.fit_one_cycle(6, max_lr=1e-02) . epoch train_loss valid_loss accuracy time . 0 | 0.437207 | 0.942094 | 0.769013 | 06:27 | . 1 | 0.374510 | 0.841367 | 0.766177 | 06:25 | . 2 | 0.191100 | 0.506396 | 0.870070 | 06:24 | . 3 | 0.079668 | 0.402396 | 0.886311 | 06:29 | . 4 | 0.059663 | 0.453178 | 0.865687 | 06:24 | . 5 | 0.029207 | 0.391314 | 0.882186 | 06:27 | . We were able to reach an accuracy of more than 88%. That&#39;s not too bad. Let&#39;s save our model. . learn.save(&#39;stage-1&#39;) . Through model training we tried to find good model parameters (i.e. weights). With the right parameters the model will be able to find characteristic features for each class. . However, we actually didn&#39;t train all of the model parameters. Why is that? Well, before using a pre-trained model the last layer of that model needs to be exchanged. Fastai does this for us. This is necessary, since the last layer of a network is specific to the classes. However, the classes of the data used to train the pre-trained model are most likely different to the classes of our problem. As a result, we definitely want to train the parameters of this new last layer to adapt the pre-trained model to our classification problem. However, at the same time the model parameters of all the other layers are kept frozen by fastai by default. . After training only the parameters of the last layer for a few epochs, we can unfreeze the parameters of all the other layers and train them as well. This procedure often works better than immediately unfreezing and training all model parameters. However, when training the whole network we shouldn&#39;t use the same maximum learning rate for all of layers. The learning rate that we found with the help of the learning rate finder should still be used for the last layer. However, the maximum learning rate for the first layers should be smaller. Why is that? Well, the first layers that try to find low level features (e.g. edges) usually require less adjustments than the last layers that try to find class specific features. As a result, fastai divides the network layers into three groups: . For the last layer group you should use the learning rate found through the learning rate finder | For the first layer group you could use e.g. a learning rate 10 or 20 times smaller than the one used by the last layer group (read more about how to set it here) | For the second layer group fastai finds a value between the learning rate of the first and the last layer group automatically | . Okay, let&#39;s unfreeze the network and train the model for 12 more epochs. . learn.unfreeze() learn.fit_one_cycle(12, max_lr=slice(1e-4,1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.157773 | 0.577920 | 0.845837 | 06:37 | . 1 | 0.187176 | 0.646526 | 0.815932 | 06:39 | . 2 | 0.166968 | 0.645245 | 0.822377 | 06:37 | . 3 | 0.137360 | 0.594758 | 0.873421 | 06:35 | . 4 | 0.148992 | 0.591642 | 0.858469 | 06:37 | . 5 | 0.095911 | 0.674571 | 0.851766 | 06:32 | . 6 | 0.065731 | 0.527851 | 0.853828 | 06:34 | . 7 | 0.053018 | 0.345897 | 0.916989 | 06:35 | . 8 | 0.017212 | 0.406156 | 0.894818 | 06:33 | . 9 | 0.010215 | 0.266608 | 0.939417 | 06:33 | . 10 | 0.006463 | 0.314194 | 0.936582 | 06:36 | . 11 | 0.008814 | 0.336284 | 0.932972 | 06:36 | . We were able to reach an accuracy of more than 93%. We could improve our model even further! Let&#39;s save the model. . learn.save(&#39;stage-2&#39;) . Model Evaluation . Now, let&#39;s evaluate our model. What mistakes does our model make? In which cases is it most wrong? To find this out we can use the ClassificationInterpretation class offered by fastai. . interp = ClassificationInterpretation.from_learner(learn) losses, idxs = interp.top_losses() len(data.valid_ds) == len(losses) == len(idxs) . True . Let&#39;s print the images with the top losses first. These are the image our model is most wrong about. . interp.plot_top_losses(9, figsize=(15,11)) . As we can see our model mostly seems to have problems to distinguish between the classes c0 (safe driving) and c9 (talking to passenger). However, when looking at the images, this seems to be a difficult task even for humans. Since the driver does not need to look at the passanger while talking to him, it is difficult to tell if he really talks to him or not. . After getting this first impression of our model, let&#39;s look at the confusion matrix. . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Alternatively, we can also print the results of the confusion matrix as a list in the following way. . interp.most_confused(min_val=2) . [(&#39;c9&#39;, &#39;c0&#39;, 153), (&#39;c9&#39;, &#39;c8&#39;, 23), (&#39;c1&#39;, &#39;c8&#39;, 21), (&#39;c0&#39;, &#39;c8&#39;, 13), (&#39;c8&#39;, &#39;c9&#39;, 13), (&#39;c8&#39;, &#39;c2&#39;, 5), (&#39;c1&#39;, &#39;c3&#39;, 4), (&#39;c8&#39;, &#39;c6&#39;, 4), (&#39;c0&#39;, &#39;c5&#39;, 2), (&#39;c2&#39;, &#39;c8&#39;, 2), (&#39;c5&#39;, &#39;c0&#39;, 2), (&#39;c7&#39;, &#39;c0&#39;, 2), (&#39;c9&#39;, &#39;c5&#39;, 2), (&#39;c9&#39;, &#39;c7&#39;, 2)] . Here we can see again that our model really seems to have most problems with class c0 and c9. However, despite this issue our model seems to be quite good already. . And we are done! In another blog post I want show how to finally predict the classes for our test set using our model and how to submit the resulting predictions to Kaggle. .",
            "url": "https://bam098.github.io/ai-blog/kaggle/image%20classification/fastai/cnn/2020/03/23/distracted-driver-detection.html",
            "relUrl": "/kaggle/image%20classification/fastai/cnn/2020/03/23/distracted-driver-detection.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Download Datasets from Kaggle on Google Colab",
            "content": "Google Colab is a platform on which you can run GPU (and TPU) accelerated programs in a jupyter-notebook like environment. As a result it is ideal for machine learning education and basic research. The platform is free to use and it has tensorflow and fastai pre-installed. . However, before we can train any machine learning models we need to get data. Kaggle is a platform from which you can download a lot of different datasets, that can be used for machine learning. In this blog post I want to show how to download data from Kaggle on Google Colab. It consists of the following steps: . Set up the Kaggle API | Download the data | Set up the Kaggle API . There are several ways to download data from Kaggle. An easy way is to use the Kaggle API. To set up the Kaggle API on Google Colab we need to run several steps. First of all we need a Kaggle API token. If you already have one, you can simply use it. However, if you do not have one or you want to create a new one, you need to do the following: . You need to log into Kaggle and go to My Account. Then scroll down to the API section and click on Expire API Token to remove previous tokens. | Then click on Create New API Token. It will download a kaggle.json file on your local machine. | . When you have the kaggle.json file, you can set up Kaggle on Google Colab. Therefor log into Google Colab and create a new notebook there. Then you need to execute the following steps. . First of all we need to install the kaggle package on Google Colab. Therefor run the following code in a Google Colab cell. . ! pip install -q kaggle . . Next we need to upload the kaggle.json file. We can do this by running the following code, which will trigger a prompt that let&#39;s you upload a file. . from google.colab import files files.upload() . Since all the data we upload to Google Colab is lost after closing Google Colab, we need to save all data to our Google Drive. To mount our Google Drive space we need to run the following code. Goolge Colab will ask you to enter an authorization code. You can get one by clicking the corresponding link that appears after running the code. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Go to this URL in a browser: &lt;URL&gt; Enter your authorization code: ·········· Mounted at /content/gdrive . The home folder of your Google Drive is located under /content/gdrive/My Drive. We create a folder named .kaggle in that home folder. There we want to store the kaggle.json file. . ! mkdir /content/gdrive/My Drive/.kaggle/ . After creating the .kaggle folder we can move the kaggle.json file there. . ! mv kaggle.json /content/gdrive/My Drive/.kaggle/ . Change the permisson of the file. . ! chmod 600 /content/gdrive/My Drive/.kaggle/kaggle.json . However, the kaggle.json file is actually not in the correct location. The kaggle package looks for it under /root. I just wanted to store it in the home folder of our Google Drive, so that it is not lost after we close Google Colab. As a result we do not need to upload the kaggle.json file again next time we want to download data from Kaggle. We can simply copy it from the .kaggle folder in our Google Drive home folder instead. . Let&#39;s copy the file from there to /root now. . ! cp -r /content/gdrive/My Drive/.kaggle/ /root/ . Now everything should be ready. However, I had some issues with the kaggle package in the following. It did not seem to be installed correctly. If you get the same problem, you can usually solve it by re-installing the package. I used version 1.5.6 of the package here. If you need another version, you can look up which one you need here. Then simply adjust the following command by replacing 1.5.6 with the version number you picked. If you do not have any problems in the following, you can skip this step. . ! pip uninstall -y kaggle ! pip install --upgrade pip ! pip install kaggle==1.5.6 ! kaggle -v . Kaggle API 1.5.6 . That&#39;s it! To check if we set up the Kaggle API correctly, we can run the following command. You should be able to see a list of the Kaggle datasets as output. If you have the a problem with the kaggle package as mentioned above, you need to re-install it as described there. . ! kaggle datasets list . ref title size lastUpdated downloadCount voteCount usabilityRating - -- -- - - sudalairajkumar/novel-corona-virus-2019-dataset Novel Corona Virus 2019 Dataset 338KB 2020-03-09 05:24:05 27345 1140 0.9705882 kimjihoo/coronavirusdataset Coronavirus-Dataset 29KB 2020-03-09 15:20:21 8220 405 1.0 rupals/gpu-runtime Segmentation GPU Kernel Performance Dataset 4MB 2020-03-01 10:04:27 115 8 0.8235294 anlthms/dfdc-video-faces DFDC video face crops - parts 4-8 2GB 2020-03-03 01:22:58 43 7 0.5 prakrutchauhan/indian-candidates-for-general-election-2019 Indian Candidates for General Election 2019 133KB 2020-03-03 07:01:53 471 33 0.7058824 brunotly/foreign-exchange-rates-per-dollar-20002019 Foreign Exchange Rates 2000-2019 1MB 2020-03-03 17:43:07 642 27 1.0 shivamb/real-or-fake-fake-jobposting-prediction [Real or Fake] Fake JobPosting Prediction 16MB 2020-02-29 08:23:34 797 50 1.0 tapakah68/yandextoloka-water-meters-dataset Water Meters Dataset 982MB 2020-02-29 10:59:49 113 11 0.9411765 shank885/knife-dataset Knife Dataset 1MB 2020-03-02 06:43:53 136 8 0.8125 imdevskp/sars-outbreak-2003-complete-dataset SARS 2003 Outbreak Complete Dataset 10KB 2020-02-26 10:25:22 650 27 1.0 imdevskp/ebola-outbreak-20142016-complete-dataset Ebola 2014-2016 Outbreak Complete Dataset 101KB 2020-02-26 14:36:31 739 33 1.0 gpiosenka/100-bird-species 130 Bird Species 899MB 2020-03-08 23:01:45 365 34 0.6875 umangjpatel/pap-smear-datasets Pap Smear Datasets 6GB 2020-03-07 11:04:23 91 9 0.875 jessemostipak/hotel-booking-demand Hotel booking demand 1MB 2020-02-13 01:27:20 7259 333 1.0 tunguz/big-five-personality-test Big Five Personality Test 159MB 2020-02-17 15:59:37 2145 161 0.9705882 arindam235/startup-investments-crunchbase StartUp Investments (Crunchbase) 3MB 2020-02-17 21:54:42 1704 106 0.88235295 brendaso/2019-coronavirus-dataset-01212020-01262020 2019 Coronavirus dataset (January - February 2020) 53KB 2020-02-06 18:09:28 7602 395 0.7352941 jamzing/sars-coronavirus-accession SARS CORONAVIRUS ACCESSION 2MB 2020-02-18 15:49:34 2107 111 0.9411765 timoboz/data-science-cheat-sheets Data Science Cheat Sheets 596MB 2020-02-04 19:42:27 3730 240 0.875 brandenciranni/democratic-debate-transcripts-2020 Democratic Debate Transcripts 2020 565KB 2020-02-27 00:07:40 536 51 1.0 . Download the Data . . Now we can download data from Kaggle. As an example I chose the distracted driver detection dataset. It has the following URL: . https://www.kaggle.com/c/state-farm-distracted-driver-detection . The name of the dataset is the last part of that URL: state-farm-distracted-driver-detection. We need the name for the command to download the data, which is shown in the following. . ! kaggle competitions download -c state-farm-distracted-driver-detection . Downloading state-farm-distracted-driver-detection.zip to /content 100% 3.99G/4.00G [00:51&lt;00:00, 55.9MB/s] 100% 4.00G/4.00G [00:51&lt;00:00, 83.9MB/s] . A state-farm-distracted-driver-detection.zip should have been downloaded. Next let&#39;s create a folder in our Google Drive in which we want to put the data. The following path already existed in my Google Drive from previous projects: /content/gdrive/ My Drive/fastai-v3/data. I decided to store the distracted driver detection dataset under this data folder as well. However, you can put the data where ever you want as long as it is in your Google Drive. . ! mkdir /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd . Move the zip file to the created folder. . ! mv state-farm-distracted-driver-detection.zip /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd . Go to that folder. !cd did not work for me here but %cd did (you can read about it here). . %cd /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd . /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd . Make sure we are in the correct folder. . ! pwd . /content/gdrive/My Drive/fastai-v3/data/state-farm-ddd . Now unzip the data in that folder. . ! unzip state-farm-distracted-driver-detection.zip -d . . Streaming output truncated to the last 5 lines. inflating: ./imgs/train/c9/img_99801.jpg inflating: ./imgs/train/c9/img_99927.jpg inflating: ./imgs/train/c9/img_9993.jpg inflating: ./imgs/train/c9/img_99949.jpg inflating: ./sample_submission.csv . The data should be there now. However, it happened to me that it was actually not immediately there after the unzipping finished. Apparently, Google Drive sometimes needs some time until the files are available. So, we do not need to do anything. We just need to wait. To check if all the files are there we should go to Google Drive from time to time and look into the folder. When I tried it, the sample_submission.csv file was always the last file. So, when this file was there, I knew all the files were there. It is important to check this directly in Google Drive and not in the folder view in Google Colab, because Google Colab seems to show the files although Google Drive does not have them yet. . . Note: You probably need to wait until all the files are available in Google Drive. . . When all the files are there, we can remove the zip file. . ! rm state-farm-distracted-driver-detection.zip . And we are finished! .",
            "url": "https://bam098.github.io/ai-blog/kaggle/google%20colab/2020/03/15/kaggle-download-colab.html",
            "relUrl": "/kaggle/google%20colab/2020/03/15/kaggle-download-colab.html",
            "date": " • Mar 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bam098.github.io/ai-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bam098.github.io/ai-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m interested in AI and IT Security. The blog1 is meant to accompany me while I’m learning about these topics. . This blog is powered by fastpages. &#8617; . |",
          "url": "https://bam098.github.io/ai-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bam098.github.io/ai-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}